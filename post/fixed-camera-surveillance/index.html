<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40.3" />
  <meta name="author" content="Karl Edwards">

  
  
  
  
    
      
    
  
  <meta name="description" content="Abstract Fixed-camera surveillance has many practical applications, including, for example: inventory protection, vehicle tolling, robot localization, personnel authentication, and construction site monitoring. Each situation has its own ideosyncracies, however, a common pre-processing task is background subtraction. Although lighting conditions may vary, and smoke, fog, or mist might obscure the view, a significant portion of the picture does not change from one frame to the next, or even from one minute to the next.">

  
  <link rel="alternate" hreflang="en-us" href="https://karledwards.github.io/post/fixed-camera-surveillance/">

  


  

  
  
  <meta name="theme-color" content="hsl(30, 90%, 68%)">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/zenburn.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://karledwards.github.io/index.xml" type="application/rss+xml" title="Unqualified Success">
  <link rel="feed" href="https://karledwards.github.io/index.xml" type="application/rss+xml" title="Unqualified Success">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://karledwards.github.io/post/fixed-camera-surveillance/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Unqualified Success">
  <meta property="og:url" content="https://karledwards.github.io/post/fixed-camera-surveillance/">
  <meta property="og:title" content="Fixed Camera Surveillance | Unqualified Success">
  <meta property="og:description" content="Abstract Fixed-camera surveillance has many practical applications, including, for example: inventory protection, vehicle tolling, robot localization, personnel authentication, and construction site monitoring. Each situation has its own ideosyncracies, however, a common pre-processing task is background subtraction. Although lighting conditions may vary, and smoke, fog, or mist might obscure the view, a significant portion of the picture does not change from one frame to the next, or even from one minute to the next.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-05-24T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-05-24T00:00:00&#43;00:00">
  

  

  <title>Fixed Camera Surveillance | Unqualified Success</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" class="dark">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Unqualified Success</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Fixed Camera Surveillance</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-05-24 00:00:00 &#43;0000 UTC" itemprop="datePublished dateModified">
      2018-05-24
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Karl Edwards">
  </span>

  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/projects">projects</a
    >, 
    
    <a href="/categories/posts">posts</a
    >
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      <div id="abstract" class="section level1">
<h1>Abstract</h1>
<p>Fixed-camera surveillance has many practical applications, including, for example: inventory protection, vehicle tolling, robot localization, personnel authentication, and construction site monitoring. Each situation has its own ideosyncracies, however, a common pre-processing task is background subtraction. Although lighting conditions may vary, and smoke, fog, or mist might obscure the view, a significant portion of the picture does not change from one frame to the next, or even from one minute to the next. By removing the portions of the image that do not change (the <strong>background</strong>), we can focus on the more interesting parts that <em>do</em> change (the <strong>foreground</strong>). This post introduces a methodology for condensing surveillance video in order to reduce the amount of time needed to find and describe events captured by fixed cameras.</p>
<hr />
<p>Get the latest version of the repository</p>
<pre class="r"><code># ( An R Package )
devtools::githubinstall( &quot;fixed-camera-surveillance&quot; )</code></pre>
<hr />
</div>
<div id="introduction" class="section level1">
<h1>1. Introduction</h1>
<p>Fixed cameras provide a way to capture a scene without requiring a person to be present. But capturing the imagery is the easy part. The challenge is to efficiently identify the interesting images so we can spend our time interpreting the events, rather than spending our time looking for events to interpret. By removing uninteresting images and presenting reviewers with only those images depicting events of interest, we can facilitate the process of documentating the time, date, and description of each event.</p>
<p>The major steps to accomplish this are:</p>
<ol style="list-style-type: decimal">
<li>Reduce the number of images by sampling the video, perhaps twice per second</li>
<li>Subtract the background from each image in the reduced set</li>
<li>Obtain the outline of each object remaining after background subtraction</li>
<li>Superimpose the refined outlines onto the images to highlight the objects</li>
<li>Present the highlighted images for evaluation by a person</li>
</ol>
</div>
<div id="related-work" class="section level1">
<h1>2. Related Work</h1>
<div id="image-processing" class="section level2">
<h2>Image Processing</h2>
<p>There are many ways to subtract the background from an image. Two common methods are summarized here.</p>
<div id="fingerprints" class="section level3">
<h3>Fingerprints</h3>
<p><span class="citation">Lampros (<a href="#ref-mlampros">2016</a>)</span> describes <em>Perceptual Hashing</em> as a way to produce a fingerprint of an image. A simplified description of one hashing algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li>Convert the input image to grayscale</li>
<li>Severely reduce the size of the image, for example to 8x8 pixels</li>
<li>Calculate the mean value of the resulting colors</li>
<li>Compare each bit of the reduced image with the mean value and set the corresponding hash bit if the color value is greater than the mean; otherwise clear the corresponding hash bit.</li>
<li>Return the hash as a hexadecimal hash string or as a set of binary features</li>
</ol>
</div>
<div id="shadows" class="section level3">
<h3>Shadows</h3>
<p><span class="citation">Kaewtrakulpong and Bowden (<a href="#ref-Kaewtrakulpong_and_Bowden">2002</a>)</span> present a more refined method that models each background pixel as a mixture of Gaussians, the weights of which represent the color stability over time, with the idea that the background will be more stable; the foreground, transient.</p>
<p>OpenCV.org has published an example implementation of this more refined method in <span class="citation">(<em>OpenCV-Python Tutorials: Video Analysis: Background Subtraction</em> <a href="#ref-OpenCV">2017</a>)</span>.</p>
<hr />
</div>
</div>
</div>
<div id="approach" class="section level1">
<h1>3. Approach</h1>
<div id="subsample-the-video" class="section level2">
<h2>3.1 Subsample the video</h2>
<p>Write a bash script that accepts one or more mp4 files and produces VLC commands to subsample the videos at an appropriate rate, resulting in approximately 120 frames per minute of input.</p>
</div>
<div id="subtract-the-background" class="section level2">
<h2>3.2 Subtract the background</h2>
<p>Evaluate Perceptual Hashing using a K-nearest neighbors classifier on a few test videos to establish a baseline measurement for separation of foreground from background. When the number of camera installations is relatively small, the fastest way to establish a starting point for the background would be to manually select a representative image from each camera. If the number of installations makes this approach too time-consuming, or if the background conditions are not stable for each installation, then it will be necessary to develop one of the more sophisticated methods for automatically selecting a background and updating it as conditions change.</p>
<p>Consider also OpenCV background subtraction ( BackgroundSubtractor and BackgroundSubtractorMOG2 ).</p>
</div>
<div id="refine-the-foreground" class="section level2">
<h2>3.3 Refine the foreground</h2>
<p>Utilize OpenCV image processing functions, such as erosion, dilation, thresholding, and smoothing to convert the predicted foreground pixels into contiguous regions representing objects.</p>
</div>
<div id="highlight-interesting-objects-and-present-the-results" class="section level2">
<h2>3.4 Highlight interesting objects and Present the results</h2>
<p>Evaluate the OpenCV integrated annotation tool for presenting predictions and soliciting feedback from humans.</p>
</div>
</div>
<div id="experimental-setup" class="section level1">
<h1>4. Experimental Setup</h1>
<div id="sample-video-at-two-frames-per-second" class="section level2">
<h2>4.1 Sample video at two frames per second</h2>
<p>The command-line interface for VLC has the following general format:</p>
<pre class="bash"><code>#  +--- invoke the VLC utility
#  |       +--- file to process
#  |       |        +--- flag to indicate that output specification follows
#  |       |        |            +--- detailed processing instructions
#  |       |        |            |               +--- pass control back to the shell
#  |       |        |            |               |
# ___ __________ ______ ___________________ __________
  vlc input_file --sout &#39;#transcode{ ... }&#39; vlc://quit</code></pre>
<p>For our test video clip, it looks like this:</p>
<pre class="bash"><code>vlc box1/clip542.mp4 --sout &#39;#transcode{ vfilter = scene{ ratio = 3, prefix = frame_, path = box2, out=dummy }, vcodec = theo, vb = 2000,scale = 1.0, acodec = none }:standard{ access = file, mux = ogg, dst = &quot;dummy.ogg&quot; }&#39; vlc://quit
</code></pre>
<p>Since many of the parameters don’t change from one video file to the next, we can use a script, <em>excerpt.sh</em>, to create the VLC commands:</p>
<pre class="bash"><code>
#    +--- the script
#    |             +--- input file
#    |             |            +--- put results here
#    |             |            |  +--- start_time_seconds (0 starts at beginning)
#    |             |            |  | +--- run_time_seconds (0 to use entire length)
#    |             |            |  | |    +--- &#39;fine&#39; or &#39;coarse&#39;
#    |             |            |  | |    |      +--- &#39;clip&#39; or &#39;frames&#39;
#    |             |            |  | |    |      |
# __________ ________________ ____ _ _ ______ ______
./excerpt.sh box1/clip542.mp4 box2 0 0 coarse frames
</code></pre>
<p>The example video is 342 seconds long, so this should produce about 684 images.</p>
<pre class="bash"><code># How many images did we make?
ls box2 | grep -c .
## 689</code></pre>
<hr />
</div>
<div id="subtract-the-background-1" class="section level2">
<h2>4.2 Subtract the background</h2>
<div id="calculate-hash-for-each-image" class="section level3">
<h3>4.2.1 Calculate hash for each image</h3>
<pre class="r"><code>require( FCS )
## Loading required package: FCS
hashes &lt;- hash_batch( DATAPATH )
## 
## time to complete : 47.65541 secs 
## 
## 
## time to complete : 22.66066 secs

# Hashes created:
hashes$names
## NULL

# Save the hashes for later
#saveRDS( hashes, paste0( DATAPATH, &#39;hashes.RDS&#39; ))
#saveRDS( hashes$names, paste0( DATAPATH, &#39;hash_names.RDS&#39; ))</code></pre>
<pre class="r"><code>
# Retrieve the saved hashes:
#hashes &lt;- readRDS( &#39;~/Dropbox/Rlibs/hashes.RDS&#39; )
#image_filenames &lt;- hashes$values[[1]]$files

f &lt;- function( i ) hashes$values[[i]]$hash
g &lt;- function( i ) hashes$names[[i]]

# Use Adaptive Density Peak Detection clustering
clusters &lt;- FCS::get_clusters( data = f(2), filenames = image_filenames )
attr(clusters$IDs,&#39;nclust&#39;)
</code></pre>
</div>
<div id="opencv-backgroundsubtractor" class="section level3">
<h3>4.2.2 OpenCV BackgroundSubtractor</h3>
</div>
<div id="opencv-backgroundsubtractormog2" class="section level3">
<h3>4.2.3 OpenCV BackgroundSubtractorMOG2</h3>
<p>.</p>
<hr />
</div>
</div>
<div id="refine-the-foreground-1" class="section level2">
<h2>4.3 Refine the foreground</h2>
<div id="erosion" class="section level3">
<h3>4.3.1 Erosion</h3>
</div>
<div id="dilation" class="section level3">
<h3>4.3.2 Dilation</h3>
</div>
<div id="thresholding" class="section level3">
<h3>4.3.3 Thresholding</h3>
</div>
<div id="smoothing" class="section level3">
<h3>4.3.4 Smoothing</h3>
<hr />
</div>
</div>
<div id="highlight-interesting-objects-and-present-the-results-1" class="section level2">
<h2>4.4 Highlight interesting objects and Present the results</h2>
<p>Evaluate the OpenCV integrated annotation tool for presenting predictions and soliciting feedback from humans.</p>
</div>
</div>
<div id="results-and-discussion" class="section level1">
<h1>5. Results and Discussion</h1>
<div id="criteria-for-success" class="section level2">
<h2>Criteria for Success</h2>
<p>In order to be successful, the new approach must save time <em>and</em> produce a summary of events that is at least as good as the current approach.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>6. Conclusion</h1>
<ul>
<li><p>What worked best?</p></li>
<li><p>How to go about setting up an operational system for processing video</p></li>
<li><p>How much time can be saved per hour of raw video input?</p></li>
<li><p>Technical Support</p></li>
<li><p>Next Steps</p></li>
</ul>
</div>
<div id="bibliography" class="section level1 unnumbered">
<h1>Bibliography</h1>
<div id="refs" class="references">
<div id="ref-Bouttefroy2010">
<p>Bouttefroy, Bouzerdoum, P. L. 2010. <em>On the Analysis of Background Subtraction Techniques Using Gaussian Mixture Models</em>. <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>. IEEE. <a href="http://ro.uow.edu.au/cgi/viewcontent.cgi?article=1820&amp;context=infopapers" class="uri">http://ro.uow.edu.au/cgi/viewcontent.cgi?article=1820&amp;context=infopapers</a>.</p>
</div>
<div id="ref-Kaewtrakulpong_and_Bowden">
<p>Kaewtrakulpong, P., and R. Bowden. 2002. “An Improved Adaptive Background Mixture Model for Real-Time Tracking with Shadow Detection.” In <em>In Proc. 2nd Eur. Workshop Adv. Video-Based Surveillance Syst</em>, 135–44. Washington, DC, USA: IEEE Computer Society. doi:<a href="https://doi.org/10.1109/ICPR.2004.479">10.1109/ICPR.2004.479</a>.</p>
</div>
<div id="ref-mlampros">
<p>Lampros, Mouselimis. 2016. “OpenImageR, an Image Processing Toolkit.” blogpost. <a href="https://www.r-bloggers.com/openimager-an-image-processing-toolkit/" class="uri">https://www.r-bloggers.com/openimager-an-image-processing-toolkit/</a>.</p>
</div>
<div id="ref-OpenCV">
<p><em>OpenCV-Python Tutorials: Video Analysis: Background Subtraction</em>. 2017. Open Source Computer Vision. <a href="https://docs.opencv.org/3.3.0/db/d5c/tutorial_py_bg_subtraction.html" class="uri">https://docs.opencv.org/3.3.0/db/d5c/tutorial_py_bg_subtraction.html</a>.</p>
</div>
<div id="ref-Power2002">
<p>Power, P. Wayne, and Johann A. Schoonees. 2002. <em>Understanding Background Mixture Models for Foreground Segmentation</em>. <em>Proceedings Image and Vision Computing New Zealand</em>. <a href="http://www.cse.psu.edu/~rtc12/CSE586Spring2010/papers/emBGsubtractAboutSandG.pdf" class="uri">http://www.cse.psu.edu/~rtc12/CSE586Spring2010/papers/emBGsubtractAboutSandG.pdf</a>.</p>
</div>
<div id="ref-ADPclust">
<p>Xu, Yifan (Ethan), and Xiao-Feng Wang. 2016. <em>ADPclust: Fast Clustering Using Adaptive Density Peak Detection</em> (version 0.7). <a href="https://cran.r-project.org/web/packages/ADPclust/ADPclust.pdf" class="uri">https://cran.r-project.org/web/packages/ADPclust/ADPclust.pdf</a>.</p>
</div>
</div>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/image-processing">image processing</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/ultimate-captioning-detector/">Ultimate Captioning Part 4</a></li>
        
        <li><a href="/post/ultimate-captioning-scorecard/">Ultimate Captioning Part 3</a></li>
        
        <li><a href="/post/ultimate-captioning-approaches/">Ultimate Captioning Part 2</a></li>
        
        <li><a href="/post/ultimate-captioning-problem-rmd/">Ultimate Captioning Part 1</a></li>
        
      </ul>
    </div>
    

    
    <div class="article-widget">
      <div class="post-nav">
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="https://karledwards.github.io/post/an-ml-pipeline-in-r/" rel="next">An ML Pipeline in R</a>
  </div>
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="https://karledwards.github.io/post/ultimate-captioning-detector/" rel="prev">Ultimate Captioning Part 4</a>
  </div>
  
</div>

    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/R.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

