<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.37" />
  <meta name="author" content="Karl Edwards">

  
  
  
  
    
      
    
  
  <meta name="description" content="How have others approached this problem? What are the key challenges in the field of video captioning? How can we decompose the problem to facilitate solution by parts? How can we leverage the work of others to save time and produce a better solution? What is most interesting, and how do people describe it? How many frames needed to determine action? Using descriptive video services to create a large data source for video annotation research.">

  
  <link rel="alternate" hreflang="en-us" href="/post/ultimate-captioning-approaches/">

  


  

  
  
  <meta name="theme-color" content="hsl(30, 90%, 68%)">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/zenburn.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Unqualified Success">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Unqualified Success">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/ultimate-captioning-approaches/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Unqualified Success">
  <meta property="og:url" content="/post/ultimate-captioning-approaches/">
  <meta property="og:title" content="Ultimate Captioning Part 2 | Unqualified Success">
  <meta property="og:description" content="How have others approached this problem? What are the key challenges in the field of video captioning? How can we decompose the problem to facilitate solution by parts? How can we leverage the work of others to save time and produce a better solution? What is most interesting, and how do people describe it? How many frames needed to determine action? Using descriptive video services to create a large data source for video annotation research.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-03-18T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-03-18T00:00:00&#43;00:00">
  

  

  <title>Ultimate Captioning Part 2 | Unqualified Success</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" class="dark">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Unqualified Success</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Ultimate Captioning Part 2</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-03-18 00:00:00 &#43;0000 UTC" itemprop="datePublished dateModified">
      2018-03-18
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Karl Edwards">
  </span>

  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/projects">projects</a
    >, 
    
    <a href="/categories/posts">posts</a
    >
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>How have others approached this problem? What are the key challenges in the field of video captioning? How can we decompose the problem to facilitate solution by parts? How can we leverage the work of others to save time and produce a better solution? What is most interesting, and how do people describe it? How many frames needed to determine action? Using descriptive video services to create a large data source for video annotation research.</p>

<ol>
<li>Where to Look For Ideas</li>
<li>What is most interesting, and how do people describe it?</li>
<li>How many frames needed to determine action?</li>
<li>Parts of the game</li>
</ol>

<ul>
<li>Separating action from the individual performing it</li>
<li>Dimensionality reduction</li>
<li>Separating action from the individual performing it</li>
<li>Unsupervised Discovery of Action Classes</li>
<li>Active Learning to Accelerate Labeling of Training Data</li>
<li>Using descriptive video services</li>
</ul>

<hr />

<h2 id="describing-key-moments-from-an-ultimate-championship-tournament">Describing Key Moments From An Ultimate Championship Tournament</h2>

<hr />

<h1 id="related-work">Related Work</h1>

<h2 id="where-to-look-for-ideas">Where to Look For Ideas</h2>

<ul>
<li>In, <strong>A Survey on Content-aware Video Analysis for Sports</strong>, <em>Huang-Chia Shih</em> introduces the the concept of the content pyramid, reviews the state-of-the-art for content hierarchies, and concludes with a discussion of remaining challenges in video analysis.</li>
</ul>

<blockquote>
<p>The <em>content pyramid</em> is presented as an alternative to the spatiotemporal approach, using semantic reasoning to reach more meaningful awareness of video content. From <em>video clips</em>, we extract <em>objects</em>, from which we infer <em>events</em>, about which we draw <em>conclusions</em>. Similarly, <em>raw data</em> become <em>named subjects and objects</em>; subjects interacting with objects in context lead to <em>understanding</em>; translating that understanding into text produces a <em>transcript</em>.</p>
</blockquote>

<h2 id="for-a-given-domain-what-is-most-interesting-and-how-do-people-describe-it">For a given domain, what is most interesting, and how do people describe it?</h2>

<ul>
<li><p>In <strong>Coherent multi-sentence video description with variable level of detail</strong>, <em>Rohrbach</em> et. al. (2014) collect and analyze a video description corpus of three levels of detail in order to understand the difference between more detailed and less detailed descriptions. They find that inferring the high level topic helps to ensure consistency across sentences, and that hand-centric features help to improve the visual recognition of object manipulation activities, which leads to corresponding improvements in the resulting descriptions. They use a two-step approach, learning first to predict a <em>semantic representation</em>, for example, ⟨ ACTIVITY, TOOL, INGREDIENT, SOURCE, TARGET ⟩, from video and then generate natural language descriptions by combining scores from a phrase-based translation model, a language model, and a distortion model. They claim that human judges rate their multi-sentence descriptions as more readable, correct, and relevant than related work.</p></li>

<li><p>In their application domain of cooking activities, it is important to describe all handled objects, so <em>Senina, Rohrbach, et. al.</em>, in their paper, <strong>Coherent Multi-Sentence Video Description with Variable Level of Detail</strong>, develop a robust hand detector, based on deformable part models, and extract color SIFT features in the vicinity of detected hands in order to facilitate the recognition of manipulated objects. They ask individuals to describe each cooking video with a single sentence, then with three to five sentences, and finally, with up to fifteen sentences, and then analyze which aspects of the video are verbalized in each case.</p></li>

<li><p>[x] Something to try: collect domain-specific descriptions of important activities at various levels of detail; develop a robust disc detector, since in this domain, the disc is central to the game.</p></li>
</ul>

<h2 id="what-is-the-minimum-number-of-frames-needed-in-order-to-determine-action">What is the minimum number of frames needed in order to determine action</h2>

<ul>
<li><p>In <strong>Action Snippets: How Many Frames Does Human Action Recognition Require?</strong>, <em>Schindler and Van Gool</em> found that 1 to 7 frames usually suffice. In their two-path model, the first path filters images at multiple scales and orientations while the second path calculates optical flow on multiple scales, directions, and speeds. The paths are combined eventually into a vector of 1000 features.</p></li>

<li><p>In, <strong>Unsupervised Discovery of Action Classes</strong>, <em>Wang et. al.</em> use the coarse shape of human figures to match pairs of images, calculating the distance between them, and then performing spectral clustering.</p></li>

<li><p>In, <strong>Action as Space-Time Shapes</strong>, <em>Blank, Gorelick, et. al.</em> regard human actions as three-dimensional shapes produced by stacking silhouettes in a space-time volume. They define a handful of continuous characteristics, including some they call <strong><em>stick</em></strong>ness, <strong><em>plate</em></strong>ness, and <strong><em>ball</em></strong>ness. Using these, and others, they produce a vector of 280 features.</p></li>

<li><p>In, <strong>Chaotic Invariants for Human Action Recognition</strong>, <em>Ali, Basharat, and Shar</em> use a method I envision as stick-figure flow. They have produced a set of 81 videos, comprising 9 actors performing 9 actions. They extract joint tracks from hands, feet, head, and belly using foreground silhouettes, and perform additional processing to produce a vector of 40 features. Their method is robust in that it finds the essential motion, independent of who performs it.</p></li>
</ul>

<h2 id="jersey-number-extraction">Jersey Number Extraction</h2>

<p>In <strong>Soccer Jersey Number Recognition Using Convolutional Neural Networks</strong>, <em>Gerke and Müller</em> find that a  holistic approach of one class per number performed better than one class per digit and that deep learning approaches yield quite good results even with smaller datasets.</p>

<h2 id="event-detection">Event Detection</h2>

<p>In, <strong>An Overview of Automatic Event Detection in Soccer Matches</strong>, <em>de Sousa Júnior</em> et. al. (2011) describe several relevant topics, including ball tracking, player tracking, kick detection, goal detection.</p>

<p>In, <strong>Automatic parsing of tv soccer programs</strong>, <em>Chuan</em> uses an <em>a priori</em> model with four components: (1) field, (2) ball, (3) players, and (4) motion. The largest green region in the picture is assumed to be the field. Field segmentation is a three-step process: (1) apply a Gaussian-Laplacian edge detector for picking line marks, remove lines whose colors are not white, and perform a thinning operation; (2) edge trimming for patching up broken edges; (3) recognize line marks. The ball is particularly hard to find, due to its small size; therefore only detected when white patch exceeds a certain size and white patch is roughly circular. To identify players, they look for distinct peaks in the color histogram and try to find two peak colors, other than green, that might be team jerseys. Once colors are established, use them for subsequent identification of players for the current game. Motion: Look for corresponding pixels in consecutive frames.</p>

<p>In, <strong>Where are the ball and players? soccer game analysis with color based tracking and image mosaic</strong>, <em>Seo et. al.</em> take a sophisticated approach translating images into a field model, using feature points when the center circle is visible and using image-based mosaicking otherwise.</p>

<h2 id="who-did-what-to-whom">Who Did What To Whom?</h2>

<h3 id="also-when-where-and-how">Also: When, Where, and How?</h3>

<p>In, <strong>Video in sentences out</strong>, <em>Barbu</em> et. al. (2015), describe an approach deeply rooted in formal semantics--<em>Meaning</em> is viewed as a relation between language and external reality, formalized in terms of reference, truth, possible worlds, etc. They criticize spatiotemporal-bags-of-words and similar approaches, claiming that these methods obtain  high accuracy for the wrong reasons, for example, associating <em>diving</em> with the blue of the pool, rather than the motion of the actor. The main idea is that <em>verbs</em> characterize interaction; participants, shapes, colors, sizes, and textures are irrelevant to action. Only after the action has been identified can these other details be used to add richness to the description. They use a limited vocabulary of 118 words, comprising 48 verbs, 24 nouns, 8 prepositions, along with other parts of speech, listed below, to generate video descriptions.</p>

<table>
<thead>
<tr>
<th align="right">Part of Speech</th>
<th align="left">Members</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">verbs</td>
<td align="left">approached, arrived, attached, bounced, buried, carried, caught, chased, closed, collided, digging, dropped, entered, exchanged, exited, fell, fled, flew, followed, gave, got, had, handed, hauled, held, hit, jumped, kicked, left, lifted, moved, opened, passed, picked, pushed, put, raised, ran, received, replaced, snatched, stopped, threw, took, touched, turned, walked, went</td>
</tr>

<tr>
<td align="right">nouns</td>
<td align="left">bag, ball, bench, bicycle, box, cage, car, cart, chair, dog, door, ladder, left, mailbox, microwave, motorcycle, object, person, right, skateboard, SUV, table, tripod, truck</td>
</tr>

<tr>
<td align="right">adjectives</td>
<td align="left">big, black, blue, cardboard, crouched, green, narrow, other, pink, prone, red, short, small, tall, teal, toy, upright, white, wide, yellow</td>
</tr>

<tr>
<td align="right">prepositions</td>
<td align="left">above, because, below, from, of, over, to, with</td>
</tr>

<tr>
<td align="right">lexical PPs</td>
<td align="left">downward, leftward, rightward, upward</td>
</tr>

<tr>
<td align="right">determiners</td>
<td align="left">an, some, that, the</td>
</tr>

<tr>
<td align="right">particles</td>
<td align="left">away, down, up</td>
</tr>

<tr>
<td align="right">pronouns</td>
<td align="left">itself, something, themselves</td>
</tr>

<tr>
<td align="right">adverbs</td>
<td align="left">quickly, slowly</td>
</tr>

<tr>
<td align="right">auxiliary</td>
<td align="left">was</td>
</tr>

<tr>
<td align="right">coordination</td>
<td align="left">and</td>
</tr>
</tbody>
</table>

<ul class="task-list">
<li><label><input type="checkbox" checked disabled class="task-list-item"> Something to try: implement their approach with a domain-specific vocabulary--ultimate verbs (pull, sky, huck, layout, etc) and ultimate nouns (disc, end-zone, defender, ...)</label></li>
</ul>

<h2 id="active-learning-to-accelerate-labeling-of-training-data">Active Learning to Accelerate Labeling of Training Data</h2>

<ul>
<li>In, <strong>Learning Active Learning from Data</strong>, <em>Konyushkova, Raphael, and Fua</em> take an iterative and adaptive approach for deciding which datapoints should be annotated next, instead of asking experts to annotate all the data.</li>
</ul>

<p>In <strong>Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition</strong>, Guadarrama et al <strong>2013</strong>, attempt to perform general, <em>broad-coverage</em>, activity recognition, beginning with subject/verb/object triplets taken from natural language descriptions of videos. Their method does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible. The semantic hierarchies learned from the data help to choose an appropriate level of generalization. They create object descriptors based on Deformable Parts Models. Their classifiers use a non-linear SVM to combine information from both object and activity features.</p>

<ul class="task-list">
<li><label><input type="checkbox" disabled class="task-list-item"> I think this is too complex, and tries to be more general than needed for the specific problem I'm trying to solve.</label></li>
</ul>

<p>in <strong>A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</strong>, <em>Das</em> et al 2013, employ a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors, and a high level module to produce final lingual descriptions.</p>

<p>The middle level is a top-down approach that detects concepts sparsely throughout the video, matches them over time, which we call stitching, and relates them to a tripartite template graph for generating language output.</p>

<p><em>Sparse Object Stitching</em> identifies the main idea from the first few images. As it considers each subsequent image, it adds additional ideas or concepts when the main idea identified from the current image is sufficiently different. So, for example, in a video showing a person, who grasps a bowl and then stirs its contents with a spoon, the sparse objects might be person, bowl, spoon.</p>

<p>The tripartite template graph takes the form (human) subject/tool/object. For a given domain, each such  triplet instantiates a manually created template, such as “⟨subject⟩ is cleaning ⟨object⟩ with ⟨tool⟩.”</p>

<ul class="task-list">
<li><label><input type="checkbox" disabled class="task-list-item"> In the Ultimate Captioning domain, there are few tools available for populating the tripartite templates--subjects might include: player, defender, observer, fan, commentator; objects might include: disc, ?, ?; but tool?</label></li>
</ul>

<p>In <strong>Sequence to sequence – video to text</strong>, <em>Venugopalan</em> et al 2015 use an end-to-end-trainable model, incorporating both intensity and optical flow inputs. Their approach requires neither an attention mechanism nor domain-specific template.</p>

<ul>
<li>Their implementation, based Caffe, is available at <a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt" target="_blank">https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt</a></li>
</ul>

<p>In <strong>Describing videos by exploiting temporal structure</strong>, Yao et al 2015 use a combination of still frame features and dynamic motion-based features for caption generation. They introduce a <em>soft-attention</em> mechanism to allow the text generating RNN to dynamically attend to specific temporal regions of the video while generating text.  Soft-attention depends both on the video representation and previous RNN state, dynamically changing through time.</p>

<p>They use a pre-trained convolutional neural network (2-D ConvNet) as well as a 3-dimensional, spatio-temporal convolutional neural network (3-D ConvNet: 2 spacial dimensions + 1 temporal dimension), pre-trained on activity-recognition datasets.</p>

<p>They point out that certain actions are inextricably dynamic. For example, given an image of a player holding a disc, what action should we predict? Holding? That is not very interesting. Just caught? About to throw? About to drop? Motion from subsequent frames will be required in order to discern the action.</p>

<h2 id="bibliography">Bibliography</h2>

<ul>
<li><p>A Survey on Content-aware Video Analysis for Sports
Huang-Chia Shih
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 99, NO. 9, JANUARY 2017
<a href="https://arxiv.org/pdf/1703.01170.pdf" target="_blank">https://arxiv.org/pdf/1703.01170.pdf</a></p></li>

<li><p>S. Gerke and K. Müller, “Soccer Jersey Number Recognition Using Convolutional Neural Networks,” in Proc. IEEE Int’l Conf. Comput. Vis., 2015, pp. 17–24</p></li>

<li><p>S.F. de Sousa Júnior, A. de A. Araújo, and D. Menotti “An overview of automatic event detection in soccer matches,” in Proc. IEEE Workshop Applicant. Comput. Vis., 2011, pp. 31–38.</p></li>
</ul>

<p>@INPROCEEDINGS{484921,
author={Yihong Gong and Lim Teck Sin and Chua Hock Chuan and Hongjiang Zhang and Masao Sakauchi},
booktitle={Proceedings of the International Conference on Multimedia Computing and Systems},
title={Automatic parsing of TV soccer programs},
year={1995},
volume={},
number={},
pages={167-174},
keywords={grammars;image matching;interactive video;multimedia computing;sport;TV soccer programs;automatic parsing;ball;domain knowledge;football;general video images;motion vectors;play categories;players;soccer court;soccer program parsing;soccer video programs;video structure;Cameras;Computer networks;Electronics industry;High performance computing;Indexing;Industrial electronics;Information retrieval;Layout;TV;Tail},
doi={10.1109/MMCS.1995.484921},
ISSN={},
month={May},}</p>

<hr />

<ul>
<li>From Y. Seo, S. Choi, H. Kim, and K.-S. Hong. Where are the ball and players? soccer game analysis with color based tracking and image mosaic. In IEEE International Conference on Image Analysis and Processing (ICIAP’97), volume II, pages 196–203, 1997: Field detection: peaks of the histograms of the RGB color space</li>
</ul>

<p>@InProceedings{10.1007/3-540-63508-4_123,
author=&quot;Seo, Yongduek
and Choi, Sunghoon
and Kim, Hyunwoo
and Hong, Ki-Sang&quot;,
editor=&quot;Del Bimbo, Alberto&quot;,
title=&quot;Where are the ball and players? Soccer game analysis with color-based tracking and image mosaick&quot;,
booktitle=&quot;Image Analysis and Processing&quot;,
year=&quot;1997&quot;,
publisher=&quot;Springer Berlin Heidelberg&quot;,
address=&quot;Berlin, Heidelberg&quot;,
pages=&quot;196--203&quot;,
abstract=&quot;Knowing the locations of the players and the ball on a ground field is important for soccer game analysis. Given an image sequence, we address three main problems: 1) ground field extraction, 2) player and ball tracking and team identification and 3) absolute player positioning. The region of ground field is extracted on the basis of color information, within which all the other processing is restricted. Players are tracked by template matching and Kalman filtering. Occlusion reasoning is done by color histogram back-projection. To find the location of a player, afield model is constructed and a transformation between the input image and the field model is computed using feature points. Otherwise, an image-based mosaicking technique is applied. Using this image-to-model transformation, the absolute positions and the trajectories of players on the field model are determined. We tested our method on real image sequences and the experimental results are given.&quot;,
isbn=&quot;978-3-540-69586-8&quot;
}</p>

<hr />

<ul>
<li>Barbu, A., Bridge, A., Burchill, Z., Coroian, D., Dickinson, S., Fidler, S., Michaux, A., Mussman, S., Narayanaswamy, S., Salvi, D., Schmidt, L., Shangguan, J., Siskind, J.M., Waggoner, J., Wang, S., Wei, J., Yin, Y., Zhang, Z.: Video in sentences out. In: Proceedings of the conference on Uncertainty in Artificial Intelligence (UAI) (2012)</li>
</ul>

<hr />

<ul>
<li>Rohrbach, A., Rohrbach, M., Qiu, W., Friedrich, A., Pinkal, M., Schiele, B.: Coherent multi-sentence video description with variable level of detail. In: Proceedings of the German Confeence on Pattern Recognition (GCPR) (2014)
@article{DBLP:journals/corr/SeninaRQFAAPS14,
author    = {Anna Senina and
           Marcus Rohrbach and
           Wei Qiu and
           Annemarie Friedrich and
           Sikandar Amin and
           Mykhaylo Andriluka and
           Manfred Pinkal and
           Bernt Schiele},
title     = {Coherent Multi-Sentence Video Description with Variable Level of Detail},
journal   = {CoRR},
volume    = {abs/1403.6173},
year      = {2014},
url       = {<a href="http://arxiv.org/abs/1403.6173" target="_blank">http://arxiv.org/abs/1403.6173</a>},
archivePrefix = {arXiv},
eprint    = {1403.6173},
timestamp = {Wed, 07 Jun 2017 14:42:05 +0200},
biburl    = {<a href="https://dblp.org/rec/bib/journals/corr/SeninaRQFAAPS14" target="_blank">https://dblp.org/rec/bib/journals/corr/SeninaRQFAAPS14</a>},
bibsource = {dblp computer science bibliography, <a href="https://dblp.org" target="_blank">https://dblp.org</a>}
}</li>
</ul>

<hr />

<p>{
  @inproceedings{conf/cvpr/SchindlerG08,
  added-at = {2014-07-31T00:00:00.000+0200},
  author = {Schindler, Konrad and Gool, Luc J. Van},
  biburl = {<a href="https://www.bibsonomy.org/bibtex/2647a24279070de2435fbae897d160a0f/dblp" target="_blank">https://www.bibsonomy.org/bibtex/2647a24279070de2435fbae897d160a0f/dblp</a>},
  booktitle = {CVPR},
  crossref = {conf/cvpr/2008},
  ee = {<a href="http://doi.ieeecomputersociety.org/10.1109/CVPR.2008.4587730" target="_blank">http://doi.ieeecomputersociety.org/10.1109/CVPR.2008.4587730</a>},
  interhash = {0528235b14d973656f1b2d645cb1309b},
  intrahash = {647a24279070de2435fbae897d160a0f},
  isbn = {978-1-4244-2242-5},
  keywords = {dblp},
  publisher = {IEEE Computer Society},
  timestamp = {2015-04-28T16:36:53.000+0200},
  title = {Action snippets: How many frames does human action recognition require?},
  url = {<a href="http://dblp.uni-trier.de/db/conf/cvpr/cvpr2008.html#SchindlerG08" target="_blank">http://dblp.uni-trier.de/db/conf/cvpr/cvpr2008.html#SchindlerG08</a>},
  year = 2008
}</p>

<p>@article{Gorelick:2007:ASS:1313054.1313284,
 author = {Gorelick, Lena and Blank, Moshe and Shechtman, Eli and Irani, Michal and Basri, Ronen},
 title = {Actions As Space-Time Shapes},
 journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
 issue_date = {December 2007},
 volume = {29},
 number = {12},
 month = dec,
 year = {2007},
 issn = {0162-8828},
 pages = {2247--2253},
 numpages = {7},
 url = {<a href="http://dx.doi.org/10.1109/TPAMI.2007.70711" target="_blank">http://dx.doi.org/10.1109/TPAMI.2007.70711</a>},
 doi = {10.1109/TPAMI.2007.70711},
 acmid = {1313284},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Action representation, Action representation, action recognition, space-time analysis, shape analysis, poisson equation, action recognition, poisson equation, shape analysis, space-time analysis},
}</p>

<p>{
@INPROCEEDINGS{4409046,
author={S. Ali and A. Basharat and M. Shah},
booktitle={2007 IEEE 11th International Conference on Computer Vision},
title={Chaotic Invariants for Human Action Recognition},
year={2007},
volume={},
number={},
pages={1-8},
keywords={Lyapunov methods;correlation methods;image recognition;integral equations;Lyapunov exponent;chaotic invariants;chaotic system;correlation dimension;correlation integral;delay-embedding;feature vector;human action modelling;human action recognition;nonlinear dynamics;Biological system modeling;Chaos;Character recognition;Computer vision;Control systems;Feature extraction;Humans;Joints;Nonlinear dynamical systems;Stochastic systems},
doi={10.1109/ICCV.2007.4409046},
ISSN={1550-5499},
month={Oct},}
}</p>

<p>{
@inproceedings{Wang:2006:UDA:1153171.1153644,
 author = {Wang, Yang and Jiang, Hao and Drew, Mark S. and Li, Ze-Nian and Mori, Greg},
 title = {Unsupervised Discovery of Action Classes},
 booktitle = {Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2},
 series = {CVPR '06},
 year = {2006},
 isbn = {0-7695-2597-0},
 pages = {1654--1661},
 numpages = {8},
 url = {<a href="http://dx.doi.org/10.1109/CVPR.2006.321" target="_blank">http://dx.doi.org/10.1109/CVPR.2006.321</a>},
 doi = {10.1109/CVPR.2006.321},
 acmid = {1153644},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}</p>

<p>@article{DBLP:journals/corr/KonyushkovaSF17,
  author    = {Ksenia Konyushkova and
               Raphael Sznitman and
               Pascal Fua},
  title     = {Learning Active Learning from Real and Synthetic Data},
  journal   = {CoRR},
  volume    = {abs/1703.03365},
  year      = {2017},
  url       = {<a href="http://arxiv.org/abs/1703.03365" target="_blank">http://arxiv.org/abs/1703.03365</a>},
  archivePrefix = {arXiv},
  eprint    = {1703.03365},
  timestamp = {Wed, 07 Jun 2017 14:40:25 +0200},
  biburl    = {<a href="https://dblp.org/rec/bib/journals/corr/KonyushkovaSF17" target="_blank">https://dblp.org/rec/bib/journals/corr/KonyushkovaSF17</a>},
  bibsource = {dblp computer science bibliography, <a href="https://dblp.org" target="_blank">https://dblp.org</a>}
}</p>

<ul>
<li><p>Guadarrama,S.,Krishnamoorthy,N.,Malkarnenkar,G.,Venugopalan,S.,Mooney, R., Darrell, T., Saenko, K.: Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition. In: Proceedings of
the IEEE International Conference on Computer Vision (ICCV) (2013)</p></li>

<li><p>Das, P., Xu, C., Doell, R., Corso, J.: Thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2013)</p></li>

<li><p>Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko, K.: Sequence to sequence – video to text. arXiv:1505.00487 (2015)</p></li>

<li><p>Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C., Larochelle, H., Courville, A.: Describing videos by exploiting temporal structure. arXiv:1502.08029v4 (2015)</p></li>
</ul>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/image-processing">image processing</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/ultimate-captioning-problem-rmd/">Ultimate Captioning Part 1</a></li>
        
      </ul>
    </div>
    

    
    <div class="article-widget">
      <div class="post-nav">
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/ultimate-captioning-scorecard/" rel="next">Ultimate Captioning Part 3</a>
  </div>
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/ultimate-captioning-problem-rmd/" rel="prev">Ultimate Captioning Part 1</a>
  </div>
  
</div>

    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/R.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

