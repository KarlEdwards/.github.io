<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.37" />
  <meta name="author" content="Karl Edwards">

  
  
  
  
    
      
    
  
  <meta name="description" content="The sport of ultimate resembles soccer in some ways. For example, both sports revolve around groups of similarly-clad individuals running around a mostly green environment, focused on a small, fast-moving object. Efforts to understand Soccer present many promising ideas for understanding ultimate.
The single image at the top of this post illustrates an action by itself: A player leaps into the air, attempting to catch the disc. Many times, motion from subsequent frames will be required in order to discern the action.">

  
  <link rel="alternate" hreflang="en-us" href="/post/ultimate-captioning-approaches/">

  


  

  
  
  <meta name="theme-color" content="hsl(30, 90%, 68%)">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/zenburn.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Unqualified Success">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Unqualified Success">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/ultimate-captioning-approaches/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Unqualified Success">
  <meta property="og:url" content="/post/ultimate-captioning-approaches/">
  <meta property="og:title" content="Ultimate Captioning Part 2 | Unqualified Success">
  <meta property="og:description" content="The sport of ultimate resembles soccer in some ways. For example, both sports revolve around groups of similarly-clad individuals running around a mostly green environment, focused on a small, fast-moving object. Efforts to understand Soccer present many promising ideas for understanding ultimate.
The single image at the top of this post illustrates an action by itself: A player leaps into the air, attempting to catch the disc. Many times, motion from subsequent frames will be required in order to discern the action."><meta property="og:image" content="/img/frame_16831.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-03-24T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-03-24T00:00:00&#43;00:00">
  

  

  <title>Ultimate Captioning Part 2 | Unqualified Success</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" class="dark">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Unqualified Success</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/frame_16831.png" class="article-banner" itemprop="image">
  

  <span class="article-header-caption">Reaching for the Disc</span>
</div>



  <div class="article-container">
    <h1 itemprop="name">Ultimate Captioning Part 2</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-03-24 00:00:00 &#43;0000 UTC" itemprop="datePublished dateModified">
      2018-03-24
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Karl Edwards">
  </span>

  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/projects">projects</a
    >, 
    
    <a href="/categories/posts">posts</a
    >
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>The sport of <em>ultimate</em> resembles soccer in some ways. For example, both sports revolve around groups of similarly-clad individuals running around a mostly green environment, focused on a small, fast-moving object. Efforts to understand Soccer present many promising ideas for understanding ultimate.</p>

<p>The single image at the top of this post illustrates an action by itself: A player leaps into the air, attempting to catch the disc. Many times, motion from subsequent frames will be required in order to discern the action. For example, given an image of a player holding a disc, what action should we predict? <em>'Holding'</em>  is not very interesting. Just caught? About to throw? About to drop? Yao et al. 2015 point out that certain actions are inextricably <em>dynamic</em>.</p>

<p>Knowing is one thing; describing is another. Once we understand the events and actions, natural language processing facilitates the generation of descriptions that reflect the way real people, who know what they are talking about, would produce.</p>

<hr />

<h2 id="describing-key-moments-from-an-ultimate-championship-tournament">Describing Key Moments From An Ultimate Championship Tournament</h2>

<hr />

<h1 id="related-work">Related Work</h1>

<h2 id="object-and-action-detection">Object and Action Detection</h2>

<ul>
<li><p>In, <strong>Unsupervised Discovery of Action Classes</strong>, <em>Wang et. al.</em> use the coarse shape of human figures to match pairs of images, calculating the distance between them, and then performing spectral clustering.</p></li>

<li><p>In, <strong>Action as Space-Time Shapes</strong>, <em>Blank, Gorelick, et. al.</em> regard human actions as three-dimensional shapes produced by stacking silhouettes in a space-time volume. They define a handful of continuous characteristics, including some they call <strong><em>stick</em></strong>ness, <strong><em>plate</em></strong>ness, and <strong><em>ball</em></strong>ness. Using these, and others, they produce a vector of 280 features.</p></li>

<li><p>In, <strong>Chaotic Invariants for Human Action Recognition</strong>, <em>Ali, Basharat, and Shar</em> use a method I envision as stick-figure flow. They have produced a set of 81 videos, comprising 9 actors performing 9 actions. They extract joint tracks from hands, feet, head, and belly using foreground silhouettes, and perform additional processing to produce a vector of 40 features. Their method is robust in that it finds the essential motion, independent of who performs it.</p></li>

<li><p>In, <strong>An Overview of Automatic Event Detection in Soccer Matches</strong>, <em>de Sousa Júnior</em> et. al. (2011) describe several relevant topics, including ball tracking, player tracking, kick detection, goal detection.</p></li>

<li><p>In, <strong>Automatic parsing of tv soccer programs</strong>, <em>Chuan</em> uses an <em>a priori</em> model with four components: (1) field, (2) ball, (3) players, and (4) motion. The largest green region in the picture is assumed to be the field. Field segmentation is a three-step process: (1) apply a Gaussian-Laplacian edge detector for picking line marks, remove lines whose colors are not white, and perform a thinning operation; (2) edge trimming for patching up broken edges; (3) recognize line marks. The ball is particularly hard to find, due to its small size; therefore only detected when white patch exceeds a certain size and white patch is roughly circular. To identify players, they look for distinct peaks in the color histogram and try to find two peak colors, other than green, that might be team jerseys. Once colors are established, use them for subsequent identification of players for the current game. Motion: Look for corresponding pixels in consecutive frames.</p></li>

<li><p>In, <strong>Where are the ball and players? soccer game analysis with color based tracking and image mosaic</strong>, <em>Seo et. al.</em> take a sophisticated approach translating images into a field model, using feature points when the center circle is visible and using image-based mosaicking otherwise.</p></li>
</ul>

<h2 id="level-of-detail">Level of Detail</h2>

<ul>
<li><p>In their application domain of cooking activities, it is important to describe all handled objects, so <em>Senina, Rohrbach, et. al.</em>, in their paper, <strong>Coherent Multi-Sentence Video Description with Variable Level of Detail</strong>, develop a robust hand detector, based on deformable part models, and extract color SIFT features in the vicinity of detected hands in order to facilitate the recognition of manipulated objects. They ask individuals to describe each cooking video with a single sentence, then with three to five sentences, and finally, with up to fifteen sentences, and then analyze which aspects of the video are verbalized in each case.</p></li>

<li><p>Things to try:</p>

<ul>
<li><p>Collect domain-specific descriptions of important activities at various levels of detail</p></li>

<li><p>Develop a robust disc detector, since in this domain, the disc is central to the game.</p></li>
</ul></li>

<li><p>In, <strong>A Survey on Content-aware Video Analysis for Sports</strong>, <em>Huang-Chia Shih</em> introduces the the concept of the content pyramid, reviews the state-of-the-art for content hierarchies, and concludes with a discussion of remaining challenges in video analysis.</p></li>
</ul>

<blockquote>
<p>The <em>content pyramid</em> is presented as an alternative to the spatiotemporal approach, using semantic reasoning to reach more meaningful awareness of video content. From <em>video clips</em>, we extract <em>objects</em>, from which we infer <em>events</em>, about which we draw <em>conclusions</em>. Similarly, <em>raw data</em> become <em>named subjects and objects</em>; subjects interacting with objects in context lead to <em>understanding</em>; translating that understanding into text produces a <em>transcript</em>.</p>
</blockquote>

<h2 id="coherent-description">Coherent Description</h2>

<h3 id="semantics">Semantics</h3>

<ul>
<li>In, <strong>Video in sentences out</strong>, <em>Barbu</em> et. al. (2015), describe an approach deeply rooted in formal semantics -- <em>Meaning</em> is viewed as a relation between language and external reality, formalized in terms of reference, truth, possible worlds, etc. They criticize spatiotemporal-bags-of-words and similar approaches, claiming that these methods obtain  high accuracy for the wrong reasons, for example, associating <em>diving</em> with the blue of the pool, rather than the motion of the actor. The main idea is that <em>verbs</em> characterize interaction; participants, shapes, colors, sizes, and textures are irrelevant to action. Only after the action has been identified can these other details be used to add richness to the description. They use a limited vocabulary of 118 words, comprising 48 verbs, 24 nouns, 8 prepositions, along with other parts of speech, listed below, to generate video descriptions.</li>
</ul>

<table>
<thead>
<tr>
<th align="right">Part of Speech</th>
<th align="left">Members</th>
</tr>
</thead>

<tbody>
<tr>
<td align="right">verbs</td>
<td align="left">approached, arrived, attached, bounced, buried, carried, caught, chased, closed, collided, digging, dropped, entered, exchanged, exited, fell, fled, flew, followed, gave, got, had, handed, hauled, held, hit, jumped, kicked, left, lifted, moved, opened, passed, picked, pushed, put, raised, ran, received, replaced, snatched, stopped, threw, took, touched, turned, walked, went</td>
</tr>

<tr>
<td align="right">nouns</td>
<td align="left">bag, ball, bench, bicycle, box, cage, car, cart, chair, dog, door, ladder, left, mailbox, microwave, motorcycle, object, person, right, skateboard, SUV, table, tripod, truck</td>
</tr>

<tr>
<td align="right">adjectives</td>
<td align="left">big, black, blue, cardboard, crouched, green, narrow, other, pink, prone, red, short, small, tall, teal, toy, upright, white, wide, yellow</td>
</tr>

<tr>
<td align="right">prepositions</td>
<td align="left">above, because, below, from, of, over, to, with</td>
</tr>

<tr>
<td align="right">lexical PPs</td>
<td align="left">downward, leftward, rightward, upward</td>
</tr>

<tr>
<td align="right">determiners</td>
<td align="left">an, some, that, the</td>
</tr>

<tr>
<td align="right">particles</td>
<td align="left">away, down, up</td>
</tr>

<tr>
<td align="right">pronouns</td>
<td align="left">itself, something, themselves</td>
</tr>

<tr>
<td align="right">adverbs</td>
<td align="left">quickly, slowly</td>
</tr>

<tr>
<td align="right">auxiliary</td>
<td align="left">was</td>
</tr>

<tr>
<td align="right">coordination</td>
<td align="left">and</td>
</tr>
</tbody>
</table>

<ul>
<li><p>Something to try:</p>

<ul>
<li>Implement the approach described by <em>Barbu</em> et. al. (2015) in <strong>Video in sentences out</strong>,  with a domain-specific vocabulary, comprising ultimate verbs (pull, sky, huck, layout) and ultimate nouns (disc, end-zone, defender)</li>
</ul></li>
</ul>

<h3 id="for-a-given-domain-what-is-most-interesting-and-how-do-people-describe-it">For a given domain, what is most interesting, and how do people describe it?</h3>

<ul>
<li>In <strong>Coherent multi-sentence video description with variable level of detail</strong>, <em>Rohrbach</em> et. al. (2014) collect and analyze a video description corpus of three levels of detail in order to understand the difference between more detailed and less detailed descriptions. They find that inferring the high level topic helps to ensure consistency across sentences, and that hand-centric features help to improve the visual recognition of object manipulation activities, which leads to corresponding improvements in the resulting descriptions. They use a two-step approach, learning first to predict a <em>semantic representation</em>, for example, ⟨ ACTIVITY, TOOL, INGREDIENT, SOURCE, TARGET ⟩, from video and then generate natural language descriptions by combining scores from a phrase-based translation model, a language model, and a distortion model. They claim that human judges rate their multi-sentence descriptions as more readable, correct, and relevant than related work.</li>
</ul>

<h2 id="nuggets-of-insight">Nuggets of Insight</h2>

<h3 id="jersey-number-extraction">Jersey Number Extraction</h3>

<p>In <strong>Soccer Jersey Number Recognition Using Convolutional Neural Networks</strong>, <em>Gerke and Müller</em> find that a  holistic approach of one class per number performed better than one class per digit and that deep learning approaches yield quite good results even with smaller datasets.</p>

<h3 id="what-is-the-minimum-number-of-frames-needed-in-order-to-determine-action">What is the minimum number of frames needed in order to determine action</h3>

<ul>
<li>In <strong>Action Snippets: How Many Frames Does Human Action Recognition Require?</strong>, <em>Schindler and Van Gool</em> found that 1 to 7 frames usually suffice. In their two-path model, the first path filters images at multiple scales and orientations while the second path calculates optical flow on multiple scales, directions, and speeds. The paths are combined eventually into a vector of 1000 features.</li>
</ul>

<h2 id="active-learning-to-accelerate-labeling-of-training-data">Active Learning to Accelerate Labeling of Training Data</h2>

<ul>
<li><p>In, <strong>Learning Active Learning from Data</strong>, <em>Konyushkova, Raphael, and Fua</em> take an iterative and adaptive approach for deciding which datapoints should be annotated next, instead of asking experts to annotate all the data.</p></li>

<li><p>In <strong>Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition</strong>, Guadarrama et al <strong>2013</strong>, attempt to perform general, <em>broad-coverage</em>, activity recognition, beginning with subject/verb/object triplets taken from natural language descriptions of videos. Their method does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible. The semantic hierarchies learned from the data help to choose an appropriate level of generalization. They create object descriptors based on Deformable Parts Models. Their classifiers use a non-linear SVM to combine information from both object and activity features.</p></li>

<li><p>In <strong>A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</strong>, <em>Das</em> et al 2013, employ a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors, and a high level module to produce final lingual descriptions.</p>

<ul>
<li><p>The middle level is a top-down approach that detects concepts sparsely throughout the video, matches them over time, which we call stitching, and relates them to a tripartite template graph for generating language output.</p></li>

<li><p><em>Sparse Object Stitching</em> identifies the main idea from the first few images. As it considers each subsequent image, it adds additional ideas or concepts when the main idea identified from the current image is sufficiently different. So, for example, in a video showing a person, who grasps a bowl and then stirs its contents with a spoon, the sparse objects might be person, bowl, spoon.</p></li>

<li><p>The tripartite template graph takes the form (human) subject/tool/object. For a given domain, each such  triplet instantiates a manually created template, such as “⟨subject⟩ is cleaning ⟨object⟩ with ⟨tool⟩.”</p></li>
</ul></li>
</ul>

<p>How might we populate the tripartite templates for Ultimate? There are <strong>Subjects</strong> ( player, defender, observer, fan, commentator ); <strong>Objects</strong> ( disc ); but <strong>Tool</strong>? In the Ultimate Captioning domain, what are the tools?</p>

<ul>
<li><p>In <strong>Sequence to sequence – video to text</strong>, <em>Venugopalan</em> et al 2015 use an end-to-end-trainable model, incorporating both intensity and optical flow inputs. Their approach requires neither an attention mechanism nor domain-specific template.</p>

<ul>
<li>Their implementation, based Caffe, is available at <a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt" target="_blank">https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt</a></li>
</ul></li>

<li><p>In <strong>Describing videos by exploiting temporal structure</strong>, Yao et al 2015 use a combination of still frame features and dynamic motion-based features for caption generation. They introduce a <em>soft-attention</em> mechanism to allow the text generating RNN to dynamically attend to specific temporal regions of the video while generating text.  Soft-attention depends both on the video representation and previous RNN state, dynamically changing through time.</p>

<ul>
<li>They use a pre-trained convolutional neural network (2-D ConvNet) as well as a 3-dimensional, spatio-temporal convolutional neural network (3-D ConvNet: 2 spacial dimensions + 1 temporal dimension), pre-trained on activity-recognition datasets.</li>
</ul></li>
</ul>

<h1 id="an-approach">An Approach</h1>

<p>It's good to have a plan, even if that plan is bound to change. After thinking about the problem and learning how others have approached similar situations, the approach that comes to mind is to detect, and then describe, the contents of the images, refine the description, and then choose the most interesting parts.</p>

<ul>
<li>Overview</li>
<li>Data Inventory</li>
<li>Glossary</li>
<li>Related Work</li>
</ul>

<hr />

<p>You are here</p>

<hr />

<ul>
<li>Detectors

<ul>
<li>Disc</li>
<li>Field</li>
<li>Actions</li>
</ul></li>
<li>Narratives

<ul>
<li>Crude Narrative</li>
<li>Refined Narrative</li>
<li>Narrative Within the Context of a Game</li>
</ul></li>
<li>Proposed Highlights</li>
</ul>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/image-processing">image processing</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/ultimate-captioning-problem-rmd/">Ultimate Captioning Part 1</a></li>
        
      </ul>
    </div>
    

    
    <div class="article-widget">
      <div class="post-nav">
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/ultimate-captioning-scorecard/" rel="next">Ultimate Captioning Part 3</a>
  </div>
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/ultimate-captioning-problem-rmd/" rel="prev">Ultimate Captioning Part 1</a>
  </div>
  
</div>

    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/R.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

