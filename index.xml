<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Unqualified Success on Unqualified Success</title>
    <link>/</link>
    <description>Recent content in Unqualified Success on Unqualified Success</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 15 Mar 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>An ML Pipeline in R</title>
      <link>/post/an-ml-pipeline-in-r/</link>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/an-ml-pipeline-in-r/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;What is a machine learning pipeline? An oil pipeline transports petroleum from one location to another. A machine learning pipeline is similar only in the sense that we put stuff into one end and it emerges at the other end. There are also some refineries along the way, that transform the product as it passes through. In the example that follows, we put one or more video files into the inlet, and the same number of videos emerge at the outlet, each having been transformed.&lt;/p&gt;

&lt;p&gt;Imagine how the pipeline will be used. We stuff &#39;PATH/to/Video/Files/&#39; into the inlet. The pipeline needs to do two things, represented below as a simple schedule:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;task&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;list video files&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;build recipe for each&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let&#39;s say the recipe we wish to apply to each video looks something like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;task&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;extract images&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;make HoGs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;cluster the HoGs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ol&gt;
&lt;li&gt;After building the recipes, the schedule looks like this:&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;task&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;list video files&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;build recipe for each&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;extract images   VIDEO1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;make HoGs        VIDEO1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;cluster the HoGs VIDEO1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;extract images   VIDEO2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;make HoGs        VIDEO2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;cluster the HoGs VIDEO2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;...&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;...&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pending&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To keep track of the transformations and transportation the pipeline will accomplish, we will need some kind of scheduling mechanism that can do the following things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Add a task&lt;/li&gt;
&lt;li&gt;Perform the next task&lt;/li&gt;
&lt;li&gt;Update the status of the current task&lt;/li&gt;
&lt;li&gt;Show the current schedule&lt;/li&gt;
&lt;li&gt;Save the current schedule to a text file&lt;/li&gt;
&lt;li&gt;Read a schedule from a text file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to &lt;em&gt;Perform the next task&lt;/em&gt;, the mechanism must know where to find the detailed steps for each task it will encounter.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fixed Camera Surveillance</title>
      <link>/post/fixed-camera-surveillance/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/fixed-camera-surveillance/</guid>
      <description>&lt;div id=&#34;abstract&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Fixed-camera surveillance has many practical applications, including, for example: inventory protection, vehicle tolling, robot localization, personnel authentication, and construction site monitoring. Each situation has its own ideosyncracies, however, a common pre-processing task is background subtraction. Although lighting conditions may vary, and smoke, fog, or mist might obscure the view, a significant portion of the picture does not change from one frame to the next, or even from one minute to the next. By removing the portions of the image that do not change (the &lt;strong&gt;background&lt;/strong&gt;), we can focus on the more interesting parts that &lt;em&gt;do&lt;/em&gt; change (the &lt;strong&gt;foreground&lt;/strong&gt;). This post introduces a methodology for condensing surveillance video in order to reduce the amount of time needed to find and describe events captured by fixed cameras.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Get the latest version of the repository&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ( An R Package )
devtools::githubinstall( &amp;quot;fixed-camera-surveillance&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Fixed cameras provide a way to capture a scene without requiring a person to be present. But capturing the imagery is the easy part. The challenge is to efficiently identify the interesting images so we can spend our time interpreting the events, rather than spending our time looking for events to interpret. By removing uninteresting images and presenting reviewers with only those images depicting events of interest, we can facilitate the process of documentating the time, date, and description of each event.&lt;/p&gt;
&lt;p&gt;The major steps to accomplish this are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Reduce the number of images by sampling the video, perhaps twice per second&lt;/li&gt;
&lt;li&gt;Subtract the background from each image in the reduced set&lt;/li&gt;
&lt;li&gt;Obtain the outline of each object remaining after background subtraction&lt;/li&gt;
&lt;li&gt;Superimpose the refined outlines onto the images to highlight the objects&lt;/li&gt;
&lt;li&gt;Present the highlighted images for evaluation by a person&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;related-work&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Related Work&lt;/h1&gt;
&lt;div id=&#34;image-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Image Processing&lt;/h2&gt;
&lt;p&gt;There are many ways to subtract the background from an image. Two common methods are summarized here.&lt;/p&gt;
&lt;div id=&#34;fingerprints&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fingerprints&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Lampros (&lt;a href=&#34;#ref-mlampros&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; describes &lt;em&gt;Perceptual Hashing&lt;/em&gt; as a way to produce a fingerprint of an image. A simplified description of one hashing algorithm is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Convert the input image to grayscale&lt;/li&gt;
&lt;li&gt;Severely reduce the size of the image, for example to 8x8 pixels&lt;/li&gt;
&lt;li&gt;Calculate the mean value of the resulting colors&lt;/li&gt;
&lt;li&gt;Compare each bit of the reduced image with the mean value and set the corresponding hash bit if the color value is greater than the mean; otherwise clear the corresponding hash bit.&lt;/li&gt;
&lt;li&gt;Return the hash as a hexadecimal hash string or as a set of binary features&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;shadows&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Shadows&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Kaewtrakulpong and Bowden (&lt;a href=&#34;#ref-Kaewtrakulpong_and_Bowden&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt; present a more refined method that models each background pixel as a mixture of Gaussians, the weights of which represent the color stability over time, with the idea that the background will be more stable; the foreground, transient.&lt;/p&gt;
&lt;p&gt;OpenCV.org has published an example implementation of this more refined method in &lt;span class=&#34;citation&#34;&gt;(&lt;em&gt;OpenCV-Python Tutorials: Video Analysis: Background Subtraction&lt;/em&gt; &lt;a href=&#34;#ref-OpenCV&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Approach&lt;/h1&gt;
&lt;div id=&#34;subsample-the-video&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.1 Subsample the video&lt;/h2&gt;
&lt;p&gt;Write a bash script that accepts one or more mp4 files and produces VLC commands to subsample the videos at an appropriate rate, resulting in approximately 120 frames per minute of input.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subtract-the-background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.2 Subtract the background&lt;/h2&gt;
&lt;p&gt;Evaluate Perceptual Hashing using a K-nearest neighbors classifier on a few test videos to establish a baseline measurement for separation of foreground from background. When the number of camera installations is relatively small, the fastest way to establish a starting point for the background would be to manually select a representative image from each camera. If the number of installations makes this approach too time-consuming, or if the background conditions are not stable for each installation, then it will be necessary to develop one of the more sophisticated methods for automatically selecting a background and updating it as conditions change.&lt;/p&gt;
&lt;p&gt;Consider also OpenCV background subtraction ( BackgroundSubtractor and BackgroundSubtractorMOG2 ).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;refine-the-foreground&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.3 Refine the foreground&lt;/h2&gt;
&lt;p&gt;Utilize OpenCV image processing functions, such as erosion, dilation, thresholding, and smoothing to convert the predicted foreground pixels into contiguous regions representing objects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highlight-interesting-objects-and-present-the-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.4 Highlight interesting objects and Present the results&lt;/h2&gt;
&lt;p&gt;Evaluate the OpenCV integrated annotation tool for presenting predictions and soliciting feedback from humans.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;experimental-setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Experimental Setup&lt;/h1&gt;
&lt;div id=&#34;sample-video-at-two-frames-per-second&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.1 Sample video at two frames per second&lt;/h2&gt;
&lt;p&gt;The command-line interface for VLC has the following general format:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#  +--- invoke the VLC utility
#  |       +--- file to process
#  |       |        +--- flag to indicate that output specification follows
#  |       |        |            +--- detailed processing instructions
#  |       |        |            |               +--- pass control back to the shell
#  |       |        |            |               |
# ___ __________ ______ ___________________ __________
  vlc input_file --sout &amp;#39;#transcode{ ... }&amp;#39; vlc://quit&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our test video clip, it looks like this:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;vlc box1/clip542.mp4 --sout &amp;#39;#transcode{ vfilter = scene{ ratio = 3, prefix = frame_, path = box2, out=dummy }, vcodec = theo, vb = 2000,scale = 1.0, acodec = none }:standard{ access = file, mux = ogg, dst = &amp;quot;dummy.ogg&amp;quot; }&amp;#39; vlc://quit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since many of the parameters don’t change from one video file to the next, we can use a script, &lt;em&gt;excerpt.sh&lt;/em&gt;, to create the VLC commands:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;
#    +--- the script
#    |             +--- input file
#    |             |            +--- put results here
#    |             |            |  +--- start_time_seconds (0 starts at beginning)
#    |             |            |  | +--- run_time_seconds (0 to use entire length)
#    |             |            |  | |    +--- &amp;#39;fine&amp;#39; or &amp;#39;coarse&amp;#39;
#    |             |            |  | |    |      +--- &amp;#39;clip&amp;#39; or &amp;#39;frames&amp;#39;
#    |             |            |  | |    |      |
# __________ ________________ ____ _ _ ______ ______
./excerpt.sh box1/clip542.mp4 box2 0 0 coarse frames
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The example video is 342 seconds long, so this should produce about 684 images.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# How many images did we make?
ls box2 | grep -c .
## 689&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;subtract-the-background-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.2 Subtract the background&lt;/h2&gt;
&lt;div id=&#34;calculate-hash-for-each-image&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4.2.1 Calculate hash for each image&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require( FCS )
## Loading required package: FCS
hashes &amp;lt;- hash_batch( DATAPATH )
## 
## time to complete : 47.65541 secs 
## 
## 
## time to complete : 22.66066 secs

# Hashes created:
hashes$names
## NULL

# Save the hashes for later
#saveRDS( hashes, paste0( DATAPATH, &amp;#39;hashes.RDS&amp;#39; ))
#saveRDS( hashes$names, paste0( DATAPATH, &amp;#39;hash_names.RDS&amp;#39; ))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Retrieve the saved hashes:
#hashes &amp;lt;- readRDS( &amp;#39;~/Dropbox/Rlibs/hashes.RDS&amp;#39; )
#image_filenames &amp;lt;- hashes$values[[1]]$files

f &amp;lt;- function( i ) hashes$values[[i]]$hash
g &amp;lt;- function( i ) hashes$names[[i]]

# Use Adaptive Density Peak Detection clustering
clusters &amp;lt;- FCS::get_clusters( data = f(2), filenames = image_filenames )
attr(clusters$IDs,&amp;#39;nclust&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;opencv-backgroundsubtractor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4.2.2 OpenCV BackgroundSubtractor&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;opencv-backgroundsubtractormog2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4.2.3 OpenCV BackgroundSubtractorMOG2&lt;/h3&gt;
&lt;p&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;refine-the-foreground-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.3 Refine the foreground&lt;/h2&gt;
&lt;div id=&#34;erosion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4.3.1 Erosion&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;dilation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4.3.2 Dilation&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;thresholding&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4.3.3 Thresholding&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;smoothing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4.3.4 Smoothing&lt;/h3&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;highlight-interesting-objects-and-present-the-results-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.4 Highlight interesting objects and Present the results&lt;/h2&gt;
&lt;p&gt;Evaluate the OpenCV integrated annotation tool for presenting predictions and soliciting feedback from humans.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results-and-discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Results and Discussion&lt;/h1&gt;
&lt;div id=&#34;criteria-for-success&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Criteria for Success&lt;/h2&gt;
&lt;p&gt;In order to be successful, the new approach must save time &lt;em&gt;and&lt;/em&gt; produce a summary of events that is at least as good as the current approach.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. Conclusion&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What worked best?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to go about setting up an operational system for processing video&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How much time can be saved per hour of raw video input?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Technical Support&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next Steps&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Bibliography&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Bouttefroy2010&#34;&gt;
&lt;p&gt;Bouttefroy, Bouzerdoum, P. L. 2010. &lt;em&gt;On the Analysis of Background Subtraction Techniques Using Gaussian Mixture Models&lt;/em&gt;. &lt;em&gt;IEEE International Conference on Acoustics, Speech, and Signal Processing&lt;/em&gt;. IEEE. &lt;a href=&#34;http://ro.uow.edu.au/cgi/viewcontent.cgi?article=1820&amp;amp;context=infopapers&#34; class=&#34;uri&#34;&gt;http://ro.uow.edu.au/cgi/viewcontent.cgi?article=1820&amp;amp;context=infopapers&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Kaewtrakulpong_and_Bowden&#34;&gt;
&lt;p&gt;Kaewtrakulpong, P., and R. Bowden. 2002. “An Improved Adaptive Background Mixture Model for Real-Time Tracking with Shadow Detection.” In &lt;em&gt;In Proc. 2nd Eur. Workshop Adv. Video-Based Surveillance Syst&lt;/em&gt;, 135–44. Washington, DC, USA: IEEE Computer Society. doi:&lt;a href=&#34;https://doi.org/10.1109/ICPR.2004.479&#34;&gt;10.1109/ICPR.2004.479&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mlampros&#34;&gt;
&lt;p&gt;Lampros, Mouselimis. 2016. “OpenImageR, an Image Processing Toolkit.” blogpost. &lt;a href=&#34;https://www.r-bloggers.com/openimager-an-image-processing-toolkit/&#34; class=&#34;uri&#34;&gt;https://www.r-bloggers.com/openimager-an-image-processing-toolkit/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-OpenCV&#34;&gt;
&lt;p&gt;&lt;em&gt;OpenCV-Python Tutorials: Video Analysis: Background Subtraction&lt;/em&gt;. 2017. Open Source Computer Vision. &lt;a href=&#34;https://docs.opencv.org/3.3.0/db/d5c/tutorial_py_bg_subtraction.html&#34; class=&#34;uri&#34;&gt;https://docs.opencv.org/3.3.0/db/d5c/tutorial_py_bg_subtraction.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Power2002&#34;&gt;
&lt;p&gt;Power, P. Wayne, and Johann A. Schoonees. 2002. &lt;em&gt;Understanding Background Mixture Models for Foreground Segmentation&lt;/em&gt;. &lt;em&gt;Proceedings Image and Vision Computing New Zealand&lt;/em&gt;. &lt;a href=&#34;http://www.cse.psu.edu/~rtc12/CSE586Spring2010/papers/emBGsubtractAboutSandG.pdf&#34; class=&#34;uri&#34;&gt;http://www.cse.psu.edu/~rtc12/CSE586Spring2010/papers/emBGsubtractAboutSandG.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ADPclust&#34;&gt;
&lt;p&gt;Xu, Yifan (Ethan), and Xiao-Feng Wang. 2016. &lt;em&gt;ADPclust: Fast Clustering Using Adaptive Density Peak Detection&lt;/em&gt; (version 0.7). &lt;a href=&#34;https://cran.r-project.org/web/packages/ADPclust/ADPclust.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/ADPclust/ADPclust.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ultimate Captioning Posts</title>
      <link>/post/ultimate-captioning-list-of-posts/</link>
      <pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ultimate-captioning-list-of-posts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ultimate Captioning Part 5</title>
      <link>/post/ultimate-captioning-detection-failures/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ultimate-captioning-detection-failures/</guid>
      <description>&lt;p&gt;Many of the code excerpts shown here are for illustration only – they aren’t evaluated; they don’t produce the results shown. In order to make the examples most meaninful and portable, I define key constants like these:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Create a few aliases to make the intent more clear in the scripts that follow.
alias PROJECTPATH=&amp;#39;echo &amp;quot;/path/to/Projects/Video-Captioning/&amp;quot;&amp;#39;
alias CLASS_LABELS=&amp;#39;echo &amp;quot;labeled_3471.txt&amp;quot;&amp;#39;
alias KEYWORD=&amp;#39;echo &amp;quot;disc&amp;quot;&amp;#39;
alias IMAGE=&amp;#39;echo &amp;quot;frame_02881.png&amp;quot;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;div id=&#34;which-models-were-tested&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Which Models Were Tested?&lt;/h2&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Start at the base path for the project
cd `PROJECTPATH`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Make a list of models
for i in `ls -d $keyword[0-9][0-9]/*/`; do
  for j in `ls -d $i* | grep $datafile`; do
    basename $i
  done
done &amp;gt; models.tmp&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Show the number of models
sed -n $= models.tmp | sed &amp;#39;s/.*/There are &amp;amp; models in all:/&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Display a numbered list of models
awk &amp;#39;{print NR, $0;}&amp;#39; models.tmp&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;how-did-each-model-perform-on-the-most-challenging-images&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How did each model perform on the most challenging images?&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Model Id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Image Id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;x&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NOT Detected when depicted&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;x&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Detected when NOT depicted&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;x&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Detected when depicted&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;x&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Not Detected when not depicted&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;which-images-generate-the-most-not-detected-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Which images generate the most &lt;em&gt;Not Detected&lt;/em&gt; errors?&lt;/h2&gt;
&lt;div id=&#34;collage-of-the-top-10&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collage of the top 10…&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Rank&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Image Id&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;…&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;N&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;which-images-generate-the-most-false-detection-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Which images generate the most &lt;em&gt;False Detection&lt;/em&gt; errors?&lt;/h2&gt;
&lt;div id=&#34;collage-of-the-top-10-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collage of the top 10…&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Rank&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Image Id&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;…&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;N&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;frame_xxxxx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;calculations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For each image depicting a disc
&lt;ul&gt;
&lt;li&gt;for each test-pos dataset including this image
&lt;ul&gt;
&lt;li&gt;count number of models where disc is &lt;em&gt;not&lt;/em&gt; found&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;For each image &lt;strong&gt;not&lt;/strong&gt; depicting a disc
&lt;ul&gt;
&lt;li&gt;for each &lt;strong&gt;test-neg&lt;/strong&gt; dataset including this image
&lt;ul&gt;
&lt;li&gt;count number of models where disc &lt;strong&gt;is&lt;/strong&gt; found&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Create a few aliases to make the intent more clear in the scripts that follow.
projectpath=/Users/Karl/Dropbox/Projects/Video-Captioning
CLASS_LABELS=labeled_3471.txt
keyword=disc

# Make a list of all images depicting a disc
grep $keyword $projectpath/$CLASS_LABELS | awk &amp;#39;{print $1;}&amp;#39; &amp;gt; images_positive.tmp
sed -n $= images_positive.tmp | sed &amp;#39;s/.*/&amp;amp; images:/&amp;#39;
head -n 3 images_positive.tmp
echo ...

# Make a list of test-pos datasets including this image
cd $projectpath
for i in `ls -d $keyword[0-9][0-9]/*/test_pos.csv`; do   echo $i; done | sed -n $= | sed &amp;#39;s/.*/&amp;amp; datasets/&amp;#39;

# Make a list of not-detected datasets including this image
cd $projectpath
for i in `ls -d $keyword[0-9][0-9]/*/false*.csv`; do   echo $i; done | sed -n $= | sed &amp;#39;s/.*/&amp;amp; datasets/&amp;#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 531 images:
## frame_02431.png
## frame_02881.png
## frame_02941.png
## ...
## ls: disc[0-9][0-9]/*/test_pos.csv: No such file or directory
## 129 datasets&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;count-occurrences-for-this-image&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Count occurrences for THIS image&lt;/h2&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;for i in `ls -d $keyword[0-9][0-9]/*/`; do
  for j in `ls -d $i*.csv`; do
    grep -c $image $j /dev/null | sed &amp;#39;s/\/dev\/null:0//g&amp;#39; | grep :1 &amp;gt;&amp;gt; results.pos
  done
done
cat results.pos&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;$ head results.pos

disc30/cells10_bins9/not_detected.csv:1
disc30/cells10_bins9/test_pos.csv:1
disc30/cells4_bins15/not_detected.csv:1
disc30/cells4_bins15/test_pos.csv:1
disc30/cells4_bins7/not_detected.csv:1
disc30/cells4_bins7/test_pos.csv:1
disc30/cells4_bins9/not_detected.csv:1
disc30/cells4_bins9/test_pos.csv:1
disc30/cells5_bins15/not_detected.csv:1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;$ ./test.sh disc `IMAGE`
Counting...
Found this image in 33 POSitive test sets.
NOT DETECTED in 33 test sets.
FALSE DETECTION in 0 test sets.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;grep test_pos results.neg | grep -c .
7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;grep test_pos results.neg
disc50/cells10_bins9/test_pos.csv:0
disc50/cells4_bins9/test_pos.csv:0
disc50/cells5_bins9/test_pos.csv:0
disc50/cells6_bins9/test_pos.csv:0
disc50/cells7_bins9/test_pos.csv:0
disc50/cells8_bins9/test_pos.csv:0
disc50/cells9_bins9/test_pos.csv:0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a-slow-way-to-find-out-how-well-the-models-performed-on-a-given-image-is&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A slow way to find out how well the models performed on a given image is:&lt;/h2&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;./test.sh disc `IMAGE` `CLASS_LABELS`&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Counting...
frame_02881.png is a POSITIVE example
It was CORRECTLY DETETECTED 7 times
out of 40, and
it was NOT DETECTED 33 times.&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;./test.sh disc frame_46531.png `CLASS_LABELS`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Counting...
frame_46531.png is a POSITIVE example
It was CORRECTLY DETETECTED 40 times
out of 40, and
it was NOT DETECTED 0 times.&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;./test.sh disc frame_07831.png `CLASS_LABELS`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Counting...
frame_07831.png is a POSITIVE example
It was CORRECTLY DETETECTED 21 times
out of 40, and
it was NOT DETECTED 17 times.

******* What about the last one? 21 + 17 is 39, yet there are 40 models...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a-way-to-evalate-all-images-would-be&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A way to evalate &lt;em&gt;all&lt;/em&gt; images would be…&lt;/h2&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# --- Find all partitions on keyword
for i in `ls -d $keyword[0-9][0-9]/*/`; do
  for j in `ls -d $i* | grep $datafile`; do
    echo Batch=&amp;#39;&amp;quot;&amp;#39;$i&amp;#39;&amp;quot;&amp;#39;
    cat $j   &amp;lt;---------- HERE, instead of catching the contents of each $datafile, get 4 files: false_detection, not_detected, test_pos, and test_neg
  done
done &amp;gt; tmp_scores

# --- Create a Header
echo &amp;#39;Batch | TP | FP | TN | FN | AUC | Sec&amp;#39; &amp;gt; scores.txt

# --- Append each row of scores
cat tmp_scores | sed -e &amp;#39;s/,/=/g&amp;#39; -e &amp;#39;s/=/ /2&amp;#39; -e &amp;#39;s/^Time-to-optimize= seconds /Seconds=/g&amp;#39; | sed &amp;#39;s/^.*=/|/g&amp;#39; | sed -e :a -e &amp;#39;$!N;s/\n|/ |/;ta&amp;#39; -e &amp;#39;P;D&amp;#39; | sed &amp;#39;s/|\&amp;quot;/\&amp;#39;$&amp;#39;\n/g&amp;#39; | sed &amp;#39;s/\&amp;quot;//g&amp;#39; &amp;gt;&amp;gt; scores.txt

# --- Show the results
grep . scores.txt&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ultimate Captioning Part 4</title>
      <link>/post/ultimate-captioning-detector/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ultimate-captioning-detector/</guid>
      <description>&lt;div id=&#34;the-task-create-a-baseline-disc-classifier.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The task: Create a baseline disc classifier.&lt;/h4&gt;
&lt;p&gt;The basic idea is to classify the images into two buckets: those that depict a disc (positive examples), and those that do not (negative examples).&lt;/p&gt;
&lt;div id=&#34;we-will-need&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;We will need:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Data&lt;/li&gt;
&lt;li&gt;Features&lt;/li&gt;
&lt;li&gt;Classifier&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;Classifier&lt;/em&gt; will separate the data into positive and negative bins, based on &lt;em&gt;Features&lt;/em&gt; extracted from the (labeled) &lt;em&gt;Data&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data &lt;span class=&#34;math inline&#34;&gt;\(\implies\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\fbox{ Extractor }\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\implies\)&lt;/span&gt; Features&lt;/p&gt;
&lt;p&gt;Features &lt;span class=&#34;math inline&#34;&gt;\(\implies\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\fbox{ Classifier }\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\implies\)&lt;/span&gt; Predictions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;which-classifier&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Which classifier?&lt;/h5&gt;
&lt;p&gt;Scikit-learn.org has a handy flowchart, &lt;a href=&#34;http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html&#34;&gt;Choosing the Right Estimator&lt;/a&gt;, which suggests starting with a linear support vector classifier when there are fewer than 100,000 examples of labeled data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;which-feature-extractor&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Which Feature Extractor?&lt;/h5&gt;
&lt;p&gt;Let’s begin with a feature extractor known for its object-detection performance: Histograms of Oriented Gradients ( HoGs, for short ) described &lt;a href=&#34;https://pdfs.semanticscholar.org/presentation/342b/c8b55d46f822e1574e4c6fccaa0b8bfa5d3b.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;refinements&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Refinements&lt;/h5&gt;
&lt;p&gt;Later, when we want a &lt;em&gt;better&lt;/em&gt; classifer, &lt;a href=&#34;https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/&#34;&gt;this&lt;/a&gt; article will be useful.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-major-steps&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;The major steps&lt;/h5&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Label the training and testing images&lt;/li&gt;
&lt;li&gt;Extract features using HoGs&lt;/li&gt;
&lt;li&gt;Classify Images and Evaluate Performance&lt;/li&gt;
&lt;li&gt;Summarize the results&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;label-the-training-and-testing-images&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Label the training and testing images&lt;/h2&gt;
&lt;div id=&#34;a.-data-source&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A. Data Source&lt;/h3&gt;
&lt;p&gt;Data for this post comprise 3471 images, excerpted from &lt;a href=&#34;https://www.youtube.com/watch?v=ULzQS2rv34s&#34;&gt;Women’s Final from the 2015 National Championships&lt;/a&gt;, in which Boston Brute Squad takes on Seattle Riot in Frisco, Texas. The images are extracted at a rate of roughly 1 frame per second for about 1 hour.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# -----------------------------------------------
# Save video from the web using VLC:
# -----------------------------------------------

vlc -v -I rc --no-sout-audio --sout=&amp;#39;#transcode{ vcodec=h264, vb=800, scale=1 }:std{ access=file,
mux=ts, dst=/path/to/resulting/video.mp4 }&amp;#39; https://www.youtube.com/watch?v=ULzQS2rv34s/ vlc://quit

# -----------------------------------------------
# Extract an excerpt of the video as a series of images:
# -----------------------------------------------

vlc $video_file  --start-time $start_time_seconds --run-time $run_time_seconds --sout \&amp;#39;#transcode{ vfilter = scene{ ratio = $FRAME_RATIO, prefix = $FILE_PREFIX, path = $output_path, out=dummy }, vcodec = theo, vb = 2000,scale = 1.0, acodec = none }:standard{ access = file, mux = ogg, dst = \&amp;quot;dummy.ogg\&amp;quot; }\&amp;#39; vlc://quit

# See https://github.com/Video-Captioning/Practical-VLC&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-data-preparation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B. Data Preparation&lt;/h3&gt;
&lt;div id=&#34;manually&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;em&gt;Manually&lt;/em&gt;:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Create a list of images in a plain, unformatted text file, with one file name on each line&lt;/li&gt;
&lt;li&gt;Include descriptive text after the file name&lt;/li&gt;
&lt;li&gt;Multiple keywords may appear in any order on a line&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;File  Name               [ optional descriptive text ]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_20041.png&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;frame_02911.png&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_02941.png disc&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;frame_02971.png field players running&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_03001.png field players running&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;frame_03031.png blurry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_03061.png field players running&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;frame_03091.png field players running&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_03121.png field players looking_up&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;frame_03151.png disc field players running&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_03181.png field players&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;frame_03211.png disc blurry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_03241.png blurry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;frame_03271.png disc_flying&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_03301.png disc_flying field players running&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;frame_03331.png field players jumping&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;frame_03361.png field players landing&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-features-using-hogs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Extract features using HoGs&lt;/h2&gt;
&lt;p&gt;Make training and testing datasets for the keyword, &lt;em&gt;disc&lt;/em&gt;, selecting training and testing examples at random from the labeled data:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# 80% of images for training; 20% for testing
#
# partition the data ---+    +--- training fraction
#                       |    |       +--- keyword
#      the script       |    |       |          +--- class label file
#          |            |    |       |          |
# ____________________ __ _______ _______ ___________________
./object_detector.sh -p -f 0.80 -k disc -l labeled_3471.txt&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will produce a subdirectory named &lt;em&gt;disc80&lt;/em&gt; and four files containing the names of image files for both positive and negative examples.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;|
+--disc80
|       +--testing_neg.txt
|       +--testing_pos.txt
|       +--training_neg.txt
|       +--training_pos.txt
|&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s extract HoG features, using 8 cells and 15 bins.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Initiate feature extraction
#
# number of cells (windows)---+     +--- number of bins
#    eXtract features ---+    |     |       +--- keyword
#      the script        |    |     |       |        +--- training fraction
#          |             |    |     |       |        |
# ____________________  __  ____  _____  _______  _______
./object_detector.sh  -x  -w 8  -b 15  -k disc  -f 0.80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two parameters, the number of &lt;em&gt;Cells&lt;/em&gt; and the number of &lt;em&gt;Bins&lt;/em&gt;, affect the size, shape, and orientation of objects that can be detected. The dimensionality of the resulting feature set is proportional to the number of bins and to the square of the number of cells, so, for both processing time and accuracy of predictions, it is important to get these parameters at least somewhat optimized.&lt;/p&gt;
&lt;p&gt;The experiments in this post utilize OpenImageR, SimpleCV, and OpenCV – three open-source packages, which are described briefly, below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;OpenImageR &lt;span class=&#34;math inline&#34;&gt;\(\implies\)&lt;/span&gt; SimpleCV &lt;span class=&#34;math inline&#34;&gt;\(\implies\)&lt;/span&gt; OpenCV&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/OpenImageR/versions/1.0.8&#34;&gt;OpenImageR&lt;/a&gt; is an Image Processing Toolkit that takes advantage of ‘RcppArmadillo’ to speed up computationally intensive functions. The histogram of oriented gradients descriptor is a modification of the ‘findHOGFeatures’ function of the ‘SimpleCV’ computer vision platform.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://simplecv.org&#34;&gt;SimpleCV&lt;/a&gt; is an open source framework for building computer vision applications that uses libraries such as OpenCV.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://opencv.org&#34;&gt;OpenCV&lt;/a&gt; is a library of programming functions mainly aimed at real-time computer vision.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;classify-images-and-evaluate-performance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Classify Images and Evaluate Performance&lt;/h2&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Initiate image classification
#
# number of cells (windows)---+     +--- number of bins
#            classify ---+    |     |       +--- keyword
#      the script        |    |     |       |        +--- training fraction
#          |             |    |     |       |        |
# ____________________  __  ____  _____  _______  _______
./object_detector.sh  -c  -w 8  -b 15  -k disc  -f 0.80

# Feature extraction and image classification can be requested together:
./object_detector.sh  -x -c  -w 8  -b 15  -k disc  -f 0.80

for bins in 10 11 12 13 14 15; do
  ./object_detector.sh  -x -c  -w 6  -b $bins  -k disc  -f 0.80
done

for cells in 3 4 5 6 7 8 9 10 11 12 13 14 15; do
  ./object_detector.sh  -x -c  -w $cells  -b 17  -k disc  -f 0.80
done&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summarize-the-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Summarize the results&lt;/h2&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;./library/Recursive-score-collection.sh disc &amp;gt; scores.txt
./show_scores.R&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/uc4-fig1.png&#34; alt=&#34;Overall performance by cells and bins.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Overall performance by cells and bins.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;which-images-are-mis-classified&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Which images are mis-classified?&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;why-are-images-mis-classified&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why are images mis-classified?&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/uc4-fig2.png&#34; alt=&#34;False Positives by cells and bins.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;False Positives by cells and bins.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/uc4-fig3.png&#34; alt=&#34;True Negatives by cells and bins.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;True Negatives by cells and bins.&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GitHub&lt;/h2&gt;
&lt;p&gt;For details of the code behind the examples used in this post, please see the following repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Video-Captioning/HoG/blob/master/object_detector.sh&#34;&gt;Object Detection&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Video-Captioning/partition_datasets&#34;&gt;Partitioning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Video-Captioning/SVM-two-class&#34;&gt;Classification&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ultimate Captioning Part 3</title>
      <link>/post/ultimate-captioning-scorecard/</link>
      <pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ultimate-captioning-scorecard/</guid>
      <description>

&lt;h1 id=&#34;scorecard&#34;&gt;Scorecard&lt;/h1&gt;

&lt;p&gt;What are the individual tasks involved in solving the whole problem, and how effective are various approaches to these tasks?&lt;/p&gt;

&lt;h2 id=&#34;general-tasks&#34;&gt;General Tasks&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;Id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Task&lt;/th&gt;
&lt;th&gt;Approach&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Details&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;G1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Extract images from video&lt;/td&gt;
&lt;td&gt;Shell script to drive command-line VLC&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;See &lt;a href=&#34;https://github.com/Video-Captioning/Practical-VLC&#34; target=&#34;_blank&#34;&gt;Practical VLC&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;G2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Partition a set of labeled images&lt;/td&gt;
&lt;td&gt;Shell script to randomly select training and testing examples, based on a list of files and class labels&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;See &lt;a href=&#34;https://github.com/Video-Captioning/partition_datasets/blob/master/partition_data_two_class.sh&#34; target=&#34;_blank&#34;&gt;Partitioning Data&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;image-classification&#34;&gt;Image Classification&lt;/h2&gt;

&lt;p&gt;Since the field, the disc, and the players are the key ingredients for a game, tools focused on detecting these items would be a reasonable place to start.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;Id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Task&lt;/th&gt;
&lt;th&gt;Approach&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Results&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;IC1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Find discs&lt;/td&gt;
&lt;td&gt;Histograms of Oriented Gradients with linear SVM&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;IC2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Find people&lt;/td&gt;
&lt;td&gt;Histograms of Oriented Gradients with linear SVM&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;IC3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Find the field&lt;/td&gt;
&lt;td&gt;Dominant Hue with KNN clustering&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Some practical suggestions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Identify images depicting the disc. The disc is particularly hard to find, due to its small size; therefore only detected when white patch exceeds a certain size and white patch is roughly circular (and not a picture of a disk on a sign or banner)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Identify images depicting the field. Assume that the largest green region in the picture is the field.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Extract field marks from images depicting the field as follows (Field segmentation):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;apply a Gaussian-Laplacian edge detector for picking line marks

&lt;ul&gt;
&lt;li&gt;remove lines whose colors are not white&lt;/li&gt;
&lt;li&gt;perform a thinning operation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;edge trimming for patching up broken edges&lt;/li&gt;
&lt;li&gt;recognize line marks&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Establish team colors by looking for distinct peaks in the color histogram and trying to find two peak colors, other than green, that might be team jerseys. Once colors are established, use them for subsequent identification of players for the current game.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;image-classification-1&#34;&gt;Image classification&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;Id&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Task&lt;/th&gt;
&lt;th&gt;Approach&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Results&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;IC1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Identify images depicting the disc&lt;/td&gt;
&lt;td&gt;Extract HoG (Histograms of Oriented Gradients) features using various numbers of cells and 9 bins&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AUC 73% using 60% of the data for training, 40% for testing, 9 cells and 9 bins&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Player Identification&lt;/td&gt;
&lt;td&gt;Semantic Segmentation&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Player Identification&lt;/td&gt;
&lt;td&gt;Instance Segmentation&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Disc Trajectory Tracking&lt;/td&gt;
&lt;td&gt;Sparse Optical Flow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Field Identification&lt;/td&gt;
&lt;td&gt;Color Histogram, HoG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Player Action&lt;/td&gt;
&lt;td&gt;Stance Classification&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Pull Identification&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Interpret Scene&lt;/td&gt;
&lt;td&gt;Content Pyramid:  raw data -&amp;gt; named subjects and objects, interacting in context -&amp;gt; understanding -&amp;gt; transcript.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Interpret Scene&lt;/td&gt;
&lt;td&gt;Disc-centric features -&amp;gt; disc activities(throw, catch) -&amp;gt; descriptions; learn to predict ⟨ ACTIVITY, TOOL, INGREDIENT, SOURCE, TARGET ⟩, from video and then generate natural language descriptions by combining scores from a phrase-based translation model, a language model, and a distortion model.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Interpret Scene&lt;/td&gt;
&lt;td&gt;Collect domain-specific descriptions of important activities at various levels of detail; develop a robust disc detector, since in this domain, the disc is central to the game.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td&gt;Implement the method of Schindler and Van Gool, a two-path model, the first path filters images at multiple scales and orientations while the second path calculates optical flow on multiple scales, directions, and speeds. The paths are combined eventually into a vector of 1000 features.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td&gt;Use the coarse shape of human figures to match pairs of images, calculating the distance between them, and then performing spectral clustering&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td&gt;Regard human actions as three-dimensional shapes produced by stacking silhouettes in a space-time volume. They define a handful of continuous characteristics, including some they call &lt;strong&gt;&lt;em&gt;stick&lt;/em&gt;&lt;/strong&gt;ness, &lt;strong&gt;&lt;em&gt;plate&lt;/em&gt;&lt;/strong&gt;ness, and &lt;strong&gt;&lt;em&gt;ball&lt;/em&gt;&lt;/strong&gt;ness. Using these, and others, they produce a vector of 280 features&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td&gt;Extract joint tracks from hands, feet, head, and belly using foreground silhouettes, and perform additional processing to produce a vector of 40 features, representing essential motion, independent of who performs it&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Extract Jersey Numbers&lt;/td&gt;
&lt;td&gt;One class per number&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Identify images depicting the field&lt;/td&gt;
&lt;td&gt;The largest green region in the picture is assumed to be the field&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Extract field marks from images depicting the field&lt;/td&gt;
&lt;td&gt;Field segmentation: (1) apply a Gaussian-Laplacian edge detector for picking line marks, remove lines whose colors are not white, and perform a thinning operation; (2) edge trimming for patching up broken edges; (3) recognize line marks.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Establish team colors&lt;/td&gt;
&lt;td&gt;Look for distinct peaks in the color histogram and try to find two peak colors, other than green, that might be team jerseys. Once colors are established, use them for subsequent identification of players for the current game.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Describe the action in a brief sequence of video frames&lt;/td&gt;
&lt;td&gt;The main idea is that &lt;em&gt;verbs&lt;/em&gt; characterize interaction; participants, shapes, colors, sizes, and textures are irrelevant to action. Only after the action has been identified can these other details be used to add richness to the description. They use a limited vocabulary of 118 words, comprising 48 verbs, 24 nouns, 8 prepositions, along with other parts of speech, listed below, to generate video descriptions.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;General, &lt;em&gt;broad-coverage&lt;/em&gt;, activity recognition&lt;/td&gt;
&lt;td&gt;Start with subject/verb/object triplets taken from natural language descriptions of videos. Create object descriptors based on Deformable Parts Models. Combine information from both object and activity features using a non-linear SVM&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Sparse Object Stitching&lt;/td&gt;
&lt;td&gt;Identify the main idea from the first few images. Consider each subsequent image, adding additional ideas or concepts when the main idea identified from the current image is sufficiently different&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Annotate images with keywords&lt;/td&gt;
&lt;td&gt;Employ a low level multimodal latent topic model for initial keyword annotation&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Annotate images associated with concept change&lt;/td&gt;
&lt;td&gt;Use a middle level concept detectors&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Annotate short sequences&lt;/td&gt;
&lt;td&gt;Use a high level module to produce final lingual descriptions by stitching concepts to a tripartite template graph&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Generate captions&lt;/td&gt;
&lt;td&gt;Using a combination of still frame features and dynamic motion-based features with a soft-attention mechanism to allow the text generating RNN to dynamically attend to specific temporal regions of the video while generating text.&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;; The middle level is a top-down approach that detects concepts sparsely throughout the video, matches them over time, which we call stitching, and relates them to a tripartite template graph for generating language output; The tripartite template graph takes the form (human) subject/tool/object. For a given domain, each such  triplet instantiates a manually created template, such as &amp;quot;⟨subject⟩ is cleaning ⟨object⟩ with ⟨tool⟩.&amp;quot; ||&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ultimate Captioning Part 2</title>
      <link>/post/ultimate-captioning-approaches/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ultimate-captioning-approaches/</guid>
      <description>

&lt;p&gt;The sport of &lt;em&gt;ultimate&lt;/em&gt; resembles soccer in some ways. For example, both sports revolve around groups of similarly-clad individuals running around a mostly green environment, focused on a small, fast-moving object. Efforts to understand Soccer present many promising ideas for understanding ultimate.&lt;/p&gt;

&lt;p&gt;The single image at the top of this post illustrates an action by itself: A player leaps into the air, attempting to catch the disc. Many times, motion from subsequent frames will be required in order to discern the action. For example, given an image of a player holding a disc, what action should we predict? &lt;em&gt;&#39;Holding&#39;&lt;/em&gt;  is not very interesting. Just caught? About to throw? About to drop? Yao et al. 2015 point out that certain actions are inextricably &lt;em&gt;dynamic&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Knowing is one thing; describing is another. Once we understand the events and actions, natural language processing facilitates the generation of descriptions that reflect the way real people, who know what they are talking about, would produce.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;describing-key-moments-from-an-ultimate-championship-tournament&#34;&gt;Describing Key Moments From An Ultimate Championship Tournament&lt;/h2&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;

&lt;h2 id=&#34;object-and-action-detection&#34;&gt;Object and Action Detection&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In, &lt;strong&gt;Unsupervised Discovery of Action Classes&lt;/strong&gt;, &lt;em&gt;Wang et. al.&lt;/em&gt; use the coarse shape of human figures to match pairs of images, calculating the distance between them, and then performing spectral clustering.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In, &lt;strong&gt;Action as Space-Time Shapes&lt;/strong&gt;, &lt;em&gt;Blank, Gorelick, et. al.&lt;/em&gt; regard human actions as three-dimensional shapes produced by stacking silhouettes in a space-time volume. They define a handful of continuous characteristics, including some they call &lt;strong&gt;&lt;em&gt;stick&lt;/em&gt;&lt;/strong&gt;ness, &lt;strong&gt;&lt;em&gt;plate&lt;/em&gt;&lt;/strong&gt;ness, and &lt;strong&gt;&lt;em&gt;ball&lt;/em&gt;&lt;/strong&gt;ness. Using these, and others, they produce a vector of 280 features.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In, &lt;strong&gt;Chaotic Invariants for Human Action Recognition&lt;/strong&gt;, &lt;em&gt;Ali, Basharat, and Shar&lt;/em&gt; use a method I envision as stick-figure flow. They have produced a set of 81 videos, comprising 9 actors performing 9 actions. They extract joint tracks from hands, feet, head, and belly using foreground silhouettes, and perform additional processing to produce a vector of 40 features. Their method is robust in that it finds the essential motion, independent of who performs it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In, &lt;strong&gt;An Overview of Automatic Event Detection in Soccer Matches&lt;/strong&gt;, &lt;em&gt;de Sousa Júnior&lt;/em&gt; et. al. (2011) describe several relevant topics, including ball tracking, player tracking, kick detection, goal detection.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In, &lt;strong&gt;Automatic parsing of tv soccer programs&lt;/strong&gt;, &lt;em&gt;Chuan&lt;/em&gt; uses an &lt;em&gt;a priori&lt;/em&gt; model with four components: (1) field, (2) ball, (3) players, and (4) motion. The largest green region in the picture is assumed to be the field. Field segmentation is a three-step process: (1) apply a Gaussian-Laplacian edge detector for picking line marks, remove lines whose colors are not white, and perform a thinning operation; (2) edge trimming for patching up broken edges; (3) recognize line marks. The ball is particularly hard to find, due to its small size; therefore only detected when white patch exceeds a certain size and white patch is roughly circular. To identify players, they look for distinct peaks in the color histogram and try to find two peak colors, other than green, that might be team jerseys. Once colors are established, use them for subsequent identification of players for the current game. Motion: Look for corresponding pixels in consecutive frames.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In, &lt;strong&gt;Where are the ball and players? soccer game analysis with color based tracking and image mosaic&lt;/strong&gt;, &lt;em&gt;Seo et. al.&lt;/em&gt; take a sophisticated approach translating images into a field model, using feature points when the center circle is visible and using image-based mosaicking otherwise.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;level-of-detail&#34;&gt;Level of Detail&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In their application domain of cooking activities, it is important to describe all handled objects, so &lt;em&gt;Senina, Rohrbach, et. al.&lt;/em&gt;, in their paper, &lt;strong&gt;Coherent Multi-Sentence Video Description with Variable Level of Detail&lt;/strong&gt;, develop a robust hand detector, based on deformable part models, and extract color SIFT features in the vicinity of detected hands in order to facilitate the recognition of manipulated objects. They ask individuals to describe each cooking video with a single sentence, then with three to five sentences, and finally, with up to fifteen sentences, and then analyze which aspects of the video are verbalized in each case.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Things to try:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Collect domain-specific descriptions of important activities at various levels of detail&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Develop a robust disc detector, since in this domain, the disc is central to the game.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In, &lt;strong&gt;A Survey on Content-aware Video Analysis for Sports&lt;/strong&gt;, &lt;em&gt;Huang-Chia Shih&lt;/em&gt; introduces the the concept of the content pyramid, reviews the state-of-the-art for content hierarchies, and concludes with a discussion of remaining challenges in video analysis.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;The &lt;em&gt;content pyramid&lt;/em&gt; is presented as an alternative to the spatiotemporal approach, using semantic reasoning to reach more meaningful awareness of video content. From &lt;em&gt;video clips&lt;/em&gt;, we extract &lt;em&gt;objects&lt;/em&gt;, from which we infer &lt;em&gt;events&lt;/em&gt;, about which we draw &lt;em&gt;conclusions&lt;/em&gt;. Similarly, &lt;em&gt;raw data&lt;/em&gt; become &lt;em&gt;named subjects and objects&lt;/em&gt;; subjects interacting with objects in context lead to &lt;em&gt;understanding&lt;/em&gt;; translating that understanding into text produces a &lt;em&gt;transcript&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;coherent-description&#34;&gt;Coherent Description&lt;/h2&gt;

&lt;h3 id=&#34;semantics&#34;&gt;Semantics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;In, &lt;strong&gt;Video in sentences out&lt;/strong&gt;, &lt;em&gt;Barbu&lt;/em&gt; et. al. (2015), describe an approach deeply rooted in formal semantics -- &lt;em&gt;Meaning&lt;/em&gt; is viewed as a relation between language and external reality, formalized in terms of reference, truth, possible worlds, etc. They criticize spatiotemporal-bags-of-words and similar approaches, claiming that these methods obtain  high accuracy for the wrong reasons, for example, associating &lt;em&gt;diving&lt;/em&gt; with the blue of the pool, rather than the motion of the actor. The main idea is that &lt;em&gt;verbs&lt;/em&gt; characterize interaction; participants, shapes, colors, sizes, and textures are irrelevant to action. Only after the action has been identified can these other details be used to add richness to the description. They use a limited vocabulary of 118 words, comprising 48 verbs, 24 nouns, 8 prepositions, along with other parts of speech, listed below, to generate video descriptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;Part of Speech&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Members&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;verbs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;approached, arrived, attached, bounced, buried, carried, caught, chased, closed, collided, digging, dropped, entered, exchanged, exited, fell, fled, flew, followed, gave, got, had, handed, hauled, held, hit, jumped, kicked, left, lifted, moved, opened, passed, picked, pushed, put, raised, ran, received, replaced, snatched, stopped, threw, took, touched, turned, walked, went&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;nouns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;bag, ball, bench, bicycle, box, cage, car, cart, chair, dog, door, ladder, left, mailbox, microwave, motorcycle, object, person, right, skateboard, SUV, table, tripod, truck&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;adjectives&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;big, black, blue, cardboard, crouched, green, narrow, other, pink, prone, red, short, small, tall, teal, toy, upright, white, wide, yellow&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;prepositions&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;above, because, below, from, of, over, to, with&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;lexical PPs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;downward, leftward, rightward, upward&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;determiners&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;an, some, that, the&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;particles&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;away, down, up&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;pronouns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;itself, something, themselves&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;adverbs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;quickly, slowly&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;auxiliary&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;was&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;coordination&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;and&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Something to try:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implement the approach described by &lt;em&gt;Barbu&lt;/em&gt; et. al. (2015) in &lt;strong&gt;Video in sentences out&lt;/strong&gt;,  with a domain-specific vocabulary, comprising ultimate verbs (pull, sky, huck, layout) and ultimate nouns (disc, end-zone, defender)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;for-a-given-domain-what-is-most-interesting-and-how-do-people-describe-it&#34;&gt;For a given domain, what is most interesting, and how do people describe it?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;Coherent multi-sentence video description with variable level of detail&lt;/strong&gt;, &lt;em&gt;Rohrbach&lt;/em&gt; et. al. (2014) collect and analyze a video description corpus of three levels of detail in order to understand the difference between more detailed and less detailed descriptions. They find that inferring the high level topic helps to ensure consistency across sentences, and that hand-centric features help to improve the visual recognition of object manipulation activities, which leads to corresponding improvements in the resulting descriptions. They use a two-step approach, learning first to predict a &lt;em&gt;semantic representation&lt;/em&gt;, for example, ⟨ ACTIVITY, TOOL, INGREDIENT, SOURCE, TARGET ⟩, from video and then generate natural language descriptions by combining scores from a phrase-based translation model, a language model, and a distortion model. They claim that human judges rate their multi-sentence descriptions as more readable, correct, and relevant than related work.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;nuggets-of-insight&#34;&gt;Nuggets of Insight&lt;/h2&gt;

&lt;h3 id=&#34;jersey-number-extraction&#34;&gt;Jersey Number Extraction&lt;/h3&gt;

&lt;p&gt;In &lt;strong&gt;Soccer Jersey Number Recognition Using Convolutional Neural Networks&lt;/strong&gt;, &lt;em&gt;Gerke and Müller&lt;/em&gt; find that a  holistic approach of one class per number performed better than one class per digit and that deep learning approaches yield quite good results even with smaller datasets.&lt;/p&gt;

&lt;h3 id=&#34;what-is-the-minimum-number-of-frames-needed-in-order-to-determine-action&#34;&gt;What is the minimum number of frames needed in order to determine action&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;Action Snippets: How Many Frames Does Human Action Recognition Require?&lt;/strong&gt;, &lt;em&gt;Schindler and Van Gool&lt;/em&gt; found that 1 to 7 frames usually suffice. In their two-path model, the first path filters images at multiple scales and orientations while the second path calculates optical flow on multiple scales, directions, and speeds. The paths are combined eventually into a vector of 1000 features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;active-learning-to-accelerate-labeling-of-training-data&#34;&gt;Active Learning to Accelerate Labeling of Training Data&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In, &lt;strong&gt;Learning Active Learning from Data&lt;/strong&gt;, &lt;em&gt;Konyushkova, Raphael, and Fua&lt;/em&gt; take an iterative and adaptive approach for deciding which datapoints should be annotated next, instead of asking experts to annotate all the data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In &lt;strong&gt;Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition&lt;/strong&gt;, Guadarrama et al &lt;strong&gt;2013&lt;/strong&gt;, attempt to perform general, &lt;em&gt;broad-coverage&lt;/em&gt;, activity recognition, beginning with subject/verb/object triplets taken from natural language descriptions of videos. Their method does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible. The semantic hierarchies learned from the data help to choose an appropriate level of generalization. They create object descriptors based on Deformable Parts Models. Their classifiers use a non-linear SVM to combine information from both object and activity features.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In &lt;strong&gt;A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching&lt;/strong&gt;, &lt;em&gt;Das&lt;/em&gt; et al 2013, employ a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors, and a high level module to produce final lingual descriptions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The middle level is a top-down approach that detects concepts sparsely throughout the video, matches them over time, which we call stitching, and relates them to a tripartite template graph for generating language output.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Sparse Object Stitching&lt;/em&gt; identifies the main idea from the first few images. As it considers each subsequent image, it adds additional ideas or concepts when the main idea identified from the current image is sufficiently different. So, for example, in a video showing a person, who grasps a bowl and then stirs its contents with a spoon, the sparse objects might be person, bowl, spoon.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The tripartite template graph takes the form (human) subject/tool/object. For a given domain, each such  triplet instantiates a manually created template, such as “⟨subject⟩ is cleaning ⟨object⟩ with ⟨tool⟩.”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How might we populate the tripartite templates for Ultimate? There are &lt;strong&gt;Subjects&lt;/strong&gt; ( player, defender, observer, fan, commentator ); &lt;strong&gt;Objects&lt;/strong&gt; ( disc ); but &lt;strong&gt;Tool&lt;/strong&gt;? In the Ultimate Captioning domain, what are the tools?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In &lt;strong&gt;Sequence to sequence – video to text&lt;/strong&gt;, &lt;em&gt;Venugopalan&lt;/em&gt; et al 2015 use an end-to-end-trainable model, incorporating both intensity and optical flow inputs. Their approach requires neither an attention mechanism nor domain-specific template.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Their implementation, based Caffe, is available at &lt;a href=&#34;https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt&#34; target=&#34;_blank&#34;&gt;https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In &lt;strong&gt;Describing videos by exploiting temporal structure&lt;/strong&gt;, Yao et al 2015 use a combination of still frame features and dynamic motion-based features for caption generation. They introduce a &lt;em&gt;soft-attention&lt;/em&gt; mechanism to allow the text generating RNN to dynamically attend to specific temporal regions of the video while generating text.  Soft-attention depends both on the video representation and previous RNN state, dynamically changing through time.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;They use a pre-trained convolutional neural network (2-D ConvNet) as well as a 3-dimensional, spatio-temporal convolutional neural network (3-D ConvNet: 2 spacial dimensions + 1 temporal dimension), pre-trained on activity-recognition datasets.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;an-approach&#34;&gt;An Approach&lt;/h1&gt;

&lt;p&gt;It&#39;s good to have a plan, even if that plan is bound to change. After thinking about the problem and learning how others have approached similar situations, the approach that comes to mind is to detect, and then describe, the contents of the images, refine the description, and then choose the most interesting parts.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Overview&lt;/li&gt;
&lt;li&gt;Data Inventory&lt;/li&gt;
&lt;li&gt;Glossary&lt;/li&gt;
&lt;li&gt;Related Work&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;You are here&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;Detectors

&lt;ul&gt;
&lt;li&gt;Disc&lt;/li&gt;
&lt;li&gt;Field&lt;/li&gt;
&lt;li&gt;Actions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Narratives

&lt;ul&gt;
&lt;li&gt;Crude Narrative&lt;/li&gt;
&lt;li&gt;Refined Narrative&lt;/li&gt;
&lt;li&gt;Narrative Within the Context of a Game&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Proposed Highlights&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Ultimate Captioning Part 1</title>
      <link>/post/ultimate-captioning-problem-rmd/</link>
      <pubDate>Sat, 17 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ultimate-captioning-problem-rmd/</guid>
      <description>

&lt;p&gt;How can we choose the best 150 two-second plays from 10 or more hours of raw video? Preparing a highlights video from raw footage is an intensely time-consuming endeavor. This post begins with a brief introduction to the sport of Ultimate, followed by an overview of the temporal structure of a game and temporal structure of scoring a single point. The data inventory describes various sources of information about the game and the post concludes with a glossary and a short list of simplifying assumptions.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;describing-key-moments-from-an-ultimate-championship-tournament&#34;&gt;Describing Key Moments From An Ultimate Championship Tournament&lt;/h2&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;what-is-ultimate&#34;&gt;What is Ultimate?&lt;/h1&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;According to the &lt;a href=&#34;https://www.usaultimate.org/resources/officiating/rules/11th_edition_rules.aspx&#34; target=&#34;_blank&#34;&gt;rules&lt;/a&gt; as described by USA Ultimate,&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Ultimate is a non-contact disc sport played by two teams of seven players. The object of the game is to score goals. A goal is scored when a player catches any legal pass in the end zone that player is attacking. A player may not run while holding the disc. The disc is advanced by passing it to other players. The disc may be passed in any direction. Any time a pass is incomplete, a turnover occurs, resulting in an immediate change of the team in possession of the disc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;temporal-structure-of-a-game&#34;&gt;Temporal Structure of a Game&lt;/h2&gt;

&lt;p&gt;The score begins at zero-zero. One team or the other scores one point at a time. The game concludes when a team accumulates the agreed number of points (typically 15) and leads by at least two points, or the time limit is reached. Major events pertaining to the game include the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Game not begun&lt;/li&gt;
&lt;li&gt;First half

&lt;ul&gt;
&lt;li&gt;Playing&lt;/li&gt;
&lt;li&gt;Not playing&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Half-time&lt;/li&gt;
&lt;li&gt;Second half

&lt;ul&gt;
&lt;li&gt;Playing&lt;/li&gt;
&lt;li&gt;Not playing&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Game over&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;temporal-structure-of-a-point&#34;&gt;Temporal Structure of a Point&lt;/h2&gt;

&lt;p&gt;Players line up on the field, one team in each end-zone. When the defensive team signals readiness to begin, the offensive team throws the disc toward defensive team. This initial throw has a particular name: the &lt;em&gt;pull&lt;/em&gt;. While in possession of the disc, a player may not run, but must throw the disc. A team advances toward the end zone by completing a succession of passes. Required events pertaining to playing for a point include the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Players line up&lt;/li&gt;
&lt;li&gt;Signal&lt;/li&gt;
&lt;li&gt;Pull&lt;/li&gt;
&lt;li&gt;Score&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additional events, which may occur between Pull and Score:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Turnover

&lt;ul&gt;
&lt;li&gt;Disc lands on field&lt;/li&gt;
&lt;li&gt;Defense blocks a pass&lt;/li&gt;
&lt;li&gt;Defense intercepts a pass&lt;/li&gt;
&lt;li&gt;Stall count reaches 10&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Injury&lt;/li&gt;
&lt;li&gt;Foul&lt;/li&gt;
&lt;li&gt;Time-out&lt;/li&gt;
&lt;li&gt;Disc goes out of bounds&lt;/li&gt;
&lt;li&gt;Offense completes a pass, not in end-zone&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;data-inventory&#34;&gt;Data Inventory&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;Potential sources of information are not limited to images from raw video files. In addition to raw footage, finished game videos, and finished highlight reels, there may also be audio or text from various sources.&lt;/p&gt;

&lt;h2 id=&#34;images&#34;&gt;Images&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Video from one or more cameras, sometimes also slow-motion video&lt;/li&gt;
&lt;li&gt;Team roster: player name, team, jersey number, image of the player&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;audio&#34;&gt;Audio&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Commentators describing play-by-play&lt;/li&gt;
&lt;li&gt;Interviews&lt;/li&gt;
&lt;li&gt;Crowd noise&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;text&#34;&gt;Text&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Contained within images

&lt;ul&gt;
&lt;li&gt;signs or banners, the scoreboard, field markers, and player numbers or names on jerseys&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Captions applied to video during a live stream or in post-game production.&lt;/li&gt;
&lt;li&gt;Game description, for example, &lt;em&gt;Caltech vs. Stanford in semi-final at noon&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Team background: name, jersey color, city of origin, seed, and standing for home and away team&lt;/li&gt;
&lt;li&gt;Social Media -- Contemporaneous tweets typically mention scoring events and amazing plays&lt;/li&gt;
&lt;li&gt;Descriptive Video Service (DVS) Founded in 1990, DVS® pioneered access to television for viewers who are blind or visually impaired. The service provides descriptive narration of key visual elements, which is then inserted within the natural pauses in dialogue to help low-vision viewers to better understand the story.&lt;/li&gt;
&lt;li&gt;Transcribed text from announcer play-by-play.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;glossary&#34;&gt;Glossary&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;What is meant by, &lt;em&gt;Classification&lt;/em&gt;, &lt;em&gt;Detection&lt;/em&gt;, or &lt;em&gt;Captioning&lt;/em&gt;, in the context of video clips or still images?&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Term&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Image Classification&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Label each image with the class it represents, e.g., &lt;em&gt;&lt;strong&gt;this&lt;/strong&gt; image contains one or more objects of &lt;strong&gt;that&lt;/strong&gt; class&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Object Detection&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;e.g., &lt;em&gt;There is an object of &lt;strong&gt;this&lt;/strong&gt; class somewhere in this image&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Identify classes of objects&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;...&lt;em&gt;semantic&lt;/em&gt; segmentation:&lt;/td&gt;
&lt;td&gt;Label each pixel with the class of object it partially represents&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;...&lt;em&gt;instance&lt;/em&gt; segmentation:&lt;/td&gt;
&lt;td&gt;Put a bounding box around each instance of a class of objects in an image&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Image Captioning&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Describe the contents or scene depicted in the image, sometimes with rich detail and coherent paragraphs, e.g., &lt;em&gt;a person standing&lt;/em&gt;, &lt;em&gt;player 23 collides in mid-air with a defender&lt;/em&gt;, &lt;em&gt;a fan wearing a blue shirt spills his beer&lt;/em&gt;, &lt;em&gt;defensive team waits in the end zone for the pull&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Video Classification&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Label video sequences or entire videos based on contents of images, e.g.,  &lt;strong&gt;this&lt;/strong&gt; video or clip is about &lt;strong&gt;that&lt;/strong&gt; topic, &lt;em&gt;this video is about ultimate&lt;/em&gt;, &lt;em&gt;this video is about basketball&lt;/em&gt;, &lt;em&gt;this video is an interview&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Video (Event) Detection&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Identify frame index and event, e.g., in frame &lt;em&gt;n&lt;/em&gt;... &lt;em&gt;a player catches the disc&lt;/em&gt;, &lt;em&gt;half-time begins&lt;/em&gt;, &lt;em&gt;the game ends&lt;/em&gt;, &lt;em&gt;the home team scores&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Video Captioning&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Describe short video sequences in terms of dynamic information such as human actions&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;... example:&lt;/td&gt;
&lt;td&gt;frames &lt;em&gt;m&lt;/em&gt; through &lt;em&gt;n&lt;/em&gt;: a player throws the disc&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;__Video Understanding&lt;/td&gt;
&lt;td&gt;Reason about the consequences of events described in a video sequence&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;... example:&lt;/td&gt;
&lt;td&gt;The home team relied on a zone-defense in the second half of the game&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;domain-specific-assumptions&#34;&gt;Domain-Specific Assumptions&lt;/h1&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;score-vs-time&#34;&gt;Score vs. Time&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The score nearly always increases by exactly one point from beginning to end of game, with occasional exceptions for errors in reporting, when the score might jump by more than one point, or even decrease&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The shortest time from pull to score is roughly the time it takes a player to run the length of the field ( 64 meters ), so perhaps 20 seconds.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The mean time between scoring events might be more like 4 minutes, if one assumes a game of 120 minutes features 30 pulls.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;identification&#34;&gt;Identification&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Teams can be identified by jersey color&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Individual players can be identified by jersey color and number&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Official observers wear orange shirts with the word, &#39;observer&#39;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The disc is mostly white, and may contain a logo&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;what-is-most-interesting&#34;&gt;What is most interesting?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Given an image containing a disc and an image &lt;em&gt;not&lt;/em&gt; containing a disc, the former is more likely to be interesting.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>About the Blog...</title>
      <link>/post/about-the-blog-rmd/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/about-the-blog-rmd/</guid>
      <description>

&lt;h1 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h1&gt;

&lt;p&gt;I began by trying to follow the instructions from these resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Blogging with R Markdown&lt;/em&gt;, by &lt;strong&gt;Kevin Wong&lt;/strong&gt; &lt;code&gt;http://kevinfw.com/post/blogging-with-r-markdown/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Building a Blog with Blogdown and GitHub&lt;/em&gt;, by &lt;strong&gt;Tyler Clavelle&lt;/strong&gt; &lt;code&gt;https://tclavelle.github.io/blog/blogdown_github/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Announcing Blogdown&lt;/em&gt;, by &lt;strong&gt;Yihui Xie&lt;/strong&gt; &lt;code&gt;https://blog.rstudio.com/2017/09/11/announcing-blogdown/&lt;/code&gt;, along with &lt;code&gt;https://github.com/rstudio/blogdown&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Blogging with Rmarkdown, knitr, and Jekyll&lt;/em&gt;, by &lt;strong&gt;Brendan Rocks&lt;/strong&gt;, &lt;code&gt;https://www.r-bloggers.com/blogging-with-rmarkdown-knitr-and-jekyll/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Happy Git and GitHub for the useR&lt;/em&gt;, by &lt;strong&gt;Jennifer Bryan&lt;/strong&gt;,  &lt;a href=&#34;https://speakerdeck.com/jennybc/happy-git-and-github-for-the-user&#34; target=&#34;_blank&#34;&gt;https://speakerdeck.com/jennybc/happy-git-and-github-for-the-user&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After many false starts and nearly giving up, things finally started falling into place.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;the-last-thing-i-remember&#34;&gt;The last thing I remember...&lt;/h1&gt;

&lt;p&gt;...before things started working:&lt;/p&gt;

&lt;h2 id=&#34;there-was-a-problem-len-of-untyped-nil-as-evidenced-below&#34;&gt;There was a problem... &lt;code&gt;len of untyped nil&lt;/code&gt; as evidenced below:&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;[?25lBuilding sites … ERROR 2018/03/15 07:34:02 Error while rendering &amp;quot;home&amp;quot; in &amp;quot;&amp;quot;: template: theme/index.html:1:3: executing &amp;quot;theme/index.html&amp;quot; at &amp;lt;partial &amp;quot;widget_page...&amp;gt;: error calling partial: template: theme/partials/widget_page.html:23:9: executing &amp;quot;theme/partials/widget_page.html&amp;quot; at &amp;lt;partial $widget $par...&amp;gt;: error calling partial: template: theme/partials/widgets/posts.html:61:51: executing &amp;quot;theme/partials/widgets/posts.html&amp;quot; at &amp;lt;len .Params.tags&amp;gt;: error calling len: len of untyped nil&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;layouts/partials/widgets/posts.html&lt;/code&gt;, I changed the capitalization of &lt;strong&gt;params&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;from:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    {{ $params := dict &amp;quot;post&amp;quot; . }}
    {{ partial &amp;quot;post_li&amp;quot; $params }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    {{ $Params := dict &amp;quot;post&amp;quot; . }}
    {{ partial &amp;quot;post_li&amp;quot; $Params }}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;in-the-file-layouts-partials-widgets-projects-html&#34;&gt;In the file, &lt;code&gt;layouts/partials/widgets/projects.html&lt;/code&gt;...&lt;/h2&gt;

&lt;p&gt;...the first line, below, was trying to &lt;code&gt;delimit $page.Params.tags &amp;quot; &amp;quot;&lt;/code&gt;, but failing when there were no &lt;code&gt;tags&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;lt;div class=&amp;quot;col-xs-12 col-sm-6 col-md-4 col-lg-6 project-item isotope-item {{ delimit $page.Params.tags &amp;quot; &amp;quot; }}&amp;quot;&amp;gt;
    &amp;lt;div class=&amp;quot;card&amp;quot;&amp;gt;
      {{ with $project.Params.image_preview }}
      &amp;lt;a href=&amp;quot;{{ $.Scratch.Get &amp;quot;project_url&amp;quot; }}&amp;quot; title=&amp;quot;&amp;quot; class=&amp;quot;card-image hover-overlay&amp;quot;
         {{ $.Scratch.Get &amp;quot;target&amp;quot; | safeHTMLAttr }}&amp;gt;
        &amp;lt;img src=&amp;quot;{{ &amp;quot;/img/&amp;quot; | relURL }}{{ . }}&amp;quot; alt=&amp;quot;&amp;quot; class=&amp;quot;img-responsive&amp;quot;&amp;gt;
      &amp;lt;/a&amp;gt;
      {{ end }}
      &amp;lt;div class=&amp;quot;card-text&amp;quot;&amp;gt;
        &amp;lt;h4&amp;gt;&amp;lt;a href=&amp;quot;{{ $.Scratch.Get &amp;quot;project_url&amp;quot; }}&amp;quot; {{ $.Scratch.Get &amp;quot;target&amp;quot; | safeHTMLAttr }}&amp;gt;{{ .Title }}&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;
        &amp;lt;div class=&amp;quot;card-desription&amp;quot;&amp;gt;
          {{ with $project.Params.summary }}&amp;lt;p&amp;gt;{{ . | markdownify }}&amp;lt;/p&amp;gt;{{ end }}
        &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/div&amp;gt;
  {{ end }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So I enclosed the whole block between:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  {{ if isset $page.Params &amp;quot;tags&amp;quot; }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  {{ end }} # isset tags
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;resulting in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  {{ if isset $page.Params &amp;quot;tags&amp;quot; }}
    &amp;lt;div class=&amp;quot;col-xs-12 col-sm-6 col-md-4 col-lg-6 project-item isotope-item {{ delimit $page.Params.tags &amp;quot; &amp;quot; }}&amp;quot;&amp;gt;
      &amp;lt;div class=&amp;quot;card&amp;quot;&amp;gt;
        {{ with $project.Params.image_preview }}
        &amp;lt;a href=&amp;quot;{{ $.Scratch.Get &amp;quot;project_url&amp;quot; }}&amp;quot; title=&amp;quot;&amp;quot; class=&amp;quot;card-image hover-overlay&amp;quot;
           {{ $.Scratch.Get &amp;quot;target&amp;quot; | safeHTMLAttr }}&amp;gt;
          &amp;lt;img src=&amp;quot;{{ &amp;quot;/img/&amp;quot; | relURL }}{{ . }}&amp;quot; alt=&amp;quot;&amp;quot; class=&amp;quot;img-responsive&amp;quot;&amp;gt;
        &amp;lt;/a&amp;gt;
        {{ end }}
        &amp;lt;div class=&amp;quot;card-text&amp;quot;&amp;gt;
          &amp;lt;h4&amp;gt;&amp;lt;a href=&amp;quot;{{ $.Scratch.Get &amp;quot;project_url&amp;quot; }}&amp;quot; {{ $.Scratch.Get &amp;quot;target&amp;quot; | safeHTMLAttr }}&amp;gt;{{ .Title }}&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;
          &amp;lt;div class=&amp;quot;card-desription&amp;quot;&amp;gt;
            {{ with $project.Params.summary }}&amp;lt;p&amp;gt;{{ . | markdownify }}&amp;lt;/p&amp;gt;{{ end }}
          &amp;lt;/div&amp;gt;
        &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
    {{ end }}
  {{ end }} # isset tags
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Shazaam!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It started working!&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;customization-and-personalization&#34;&gt;Customization and Personalization&lt;/h1&gt;

&lt;p&gt;Next, I turned my attention to the &lt;strong&gt;About&lt;/strong&gt; page&lt;/p&gt;

&lt;h2 id=&#34;how-to-add-a-sub-section&#34;&gt;How to add a Sub-Section...&lt;/h2&gt;

&lt;p&gt;This required modification of a few files:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;about.md&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;about.html&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;en.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;about-md&#34;&gt;about.md&lt;/h2&gt;

&lt;p&gt;The TOML (&lt;code&gt;Tom&#39;s Obvious, Minimal Language&lt;/code&gt;) front matter in &lt;strong&gt;about.md&lt;/strong&gt; invites one to &#39;List your academic interests.&#39;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[interests]
  interests = [
    &amp;quot;Machine Learning&amp;quot;,
    &amp;quot;Image Processing&amp;quot;,
    &amp;quot;Video Captioning&amp;quot;
  ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I added a similar section:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[pets]
  pets = [
    &amp;quot;Rex&amp;quot;,
    &amp;quot;Fido&amp;quot;,
    &amp;quot;Rover&amp;quot;
  ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In, &lt;code&gt;about.html&lt;/code&gt;, noting the &lt;em&gt;interests&lt;/em&gt; section:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      {{ with $page.Params.interests }}
      &amp;lt;div class=&amp;quot;col-sm-5&amp;quot;&amp;gt;
        &amp;lt;h3&amp;gt;{{ i18n &amp;quot;interests&amp;quot; | markdownify }}&amp;lt;/h3&amp;gt;
        &amp;lt;ul class=&amp;quot;ul-interests&amp;quot;&amp;gt;
          {{ range .interests }}
          &amp;lt;li&amp;gt;{{ . | markdownify }}&amp;lt;/li&amp;gt;
          {{ end }}
        &amp;lt;/ul&amp;gt;
      &amp;lt;/div&amp;gt;
      {{ end }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I added a similar section for &lt;em&gt;pets&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      {{ with $page.Params.pets }}
      &amp;lt;div class=&amp;quot;col-sm-5&amp;quot;&amp;gt;
        &amp;lt;h3&amp;gt;{{ i18n &amp;quot;pets&amp;quot; | markdownify }}&amp;lt;/h3&amp;gt;
        &amp;lt;ul class=&amp;quot;ul-pets&amp;quot;&amp;gt;
          {{ range .pets }}
          &amp;lt;li&amp;gt;{{ . | markdownify }}&amp;lt;/li&amp;gt;
          {{ end }}
        &amp;lt;/ul&amp;gt;
      &amp;lt;/div&amp;gt;
      {{ end }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And, in &lt;code&gt;en.yaml&lt;/code&gt;, beneath the widget associated with the &lt;code&gt;About&lt;/code&gt; section:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# About widget

- id: interests
  translation: Interests

- id: education
  translation: Education
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I appended:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
- id: pets
  translation: Pets
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Images Like This</title>
      <link>/project/images-like-this/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/images-like-this/</guid>
      <description>

&lt;h2 id=&#34;the-general-idea&#34;&gt;The General Idea&lt;/h2&gt;

&lt;p&gt;Given a set of images, find the ones that are most similar to a set of examples.&lt;/p&gt;

&lt;h2 id=&#34;more-specifically&#34;&gt;More Specifically&lt;/h2&gt;

&lt;p&gt;Given a dataset, comprising image features extracted using a pre-trained model, and given also the row identifiers for a subset of the data, representing the kind of images to find, create a TRAINING set of the specified rows and a TEST set of all remaining rows, then train a binary SVM classifier to identify images in the TEST set, which are similar to those in the TRAINING set.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;$ python3 images_like_this.py ./data/2_images/ ./data/0_training/highfive/ ./data/3_features/features_VGG16.csv --collage Yes&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Large Integer Multiplication</title>
      <link>/project/large-integer-multiplication/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/large-integer-multiplication/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;/integer.png&#34; alt=&#34;integer&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Photogrammetric Potsherd Profile</title>
      <link>/project/photogrammetric-potsherd/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/photogrammetric-potsherd/</guid>
      <description>

&lt;h2 id=&#34;the-general-idea&#34;&gt;The General Idea&lt;/h2&gt;

&lt;p&gt;Make photogrammetry readily accessible to archaeologists through open source computational archaeology.&lt;/p&gt;

&lt;h2 id=&#34;more-specifically&#34;&gt;More Specifically&lt;/h2&gt;

&lt;p&gt;This project describes in some detail the steps to photograph a potsherd and generate a mathematical model from the resulting images, and, in much less detail, suggests how to automatically measure the object represented by the model. The R source code is available on github and an R package will be available soon. A number of simplifying assumptions accelerated this initial draft.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;post/photogrammetric-potsherd/dimensioned_profile.png&#34; alt=&#34;Resulting dimensions&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Plain Markdown Project</title>
      <link>/project/example-plain-markdown-project/</link>
      <pubDate>Sat, 09 Sep 2017 21:49:57 -0700</pubDate>
      
      <guid>/project/example-plain-markdown-project/</guid>
      <description>&lt;hr /&gt;

&lt;p&gt;Summary&lt;/p&gt;

&lt;p&gt;Example of a &lt;strong&gt;Plain Markdown&lt;/strong&gt; project and how &lt;em&gt;Plain Markdown&lt;/em&gt; differs from &lt;em&gt;R Markdown&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;one&lt;/li&gt;
&lt;li&gt;two&lt;/li&gt;
&lt;li&gt;three&lt;/li&gt;
&lt;li&gt;four&lt;/li&gt;
&lt;li&gt;five&lt;/li&gt;
&lt;li&gt;six&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;This is a post written in plain Markdown (&lt;code&gt;*.md&lt;/code&gt;) instead of R Markdown (&lt;code&gt;*.Rmd&lt;/code&gt;). The major differences are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You cannot run any R code in a plain Markdown document, whereas in an R Markdown document, you can embed R code chunks (&lt;code&gt;```{r}&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;A plain Markdown post is rendered through &lt;a href=&#34;https://gohugo.io/overview/configuration/&#34; target=&#34;_blank&#34;&gt;Blackfriday&lt;/a&gt;, and an R Markdown document is compiled by &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;rmarkdown&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;http://pandoc.org&#34; target=&#34;_blank&#34;&gt;Pandoc&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are many differences in syntax between Blackfriday&#39;s Markdown and Pandoc&#39;s Markdown. For example, you can write a task list with Blackfriday but not with Pandoc:&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Write an R package.&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Write a book.&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; ...&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Profit!&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similarly, Blackfriday does not support LaTeX math and Pandoc does. I have added the MathJax support to this theme (&lt;a href=&#34;https://github.com/yihui/hugo-lithium-theme&#34; target=&#34;_blank&#34;&gt;hugo-lithium-theme&lt;/a&gt;) but there is a caveat for plain Markdown posts: you have to include math expressions in a pair of backticks (inline: &lt;code&gt;`$ $`&lt;/code&gt;; display style: &lt;code&gt;`$$ $$`&lt;/code&gt;), e.g., &lt;code&gt;$S_n = \sum_{i=1}^n X_i$&lt;/code&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:This-is-because&#34;&gt;&lt;a href=&#34;#fn:This-is-because&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; For R Markdown posts, you do not need the backticks, because Pandoc can identify and process math expressions.&lt;/p&gt;

&lt;p&gt;When creating a new post, you have to decide whether the post format is Markdown or R Markdown, and this can be done via the &lt;code&gt;ext&lt;/code&gt; argument of the function &lt;code&gt;blogdown::new_post()&lt;/code&gt;, e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::new_post(&amp;quot;Post Title&amp;quot;, ext = &#39;.Rmd&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:This-is-because&#34;&gt;This is because we have to protect the math expressions from being interpreted as Markdown. You may not need the backticks if your math expression does not contain any special Markdown syntax such as underscores or asterisks, but it is always a safer choice to use backticks. When you happen to have a pair of literal dollar signs inside the same element, you can escape one dollar sign, e.g., &lt;code&gt;\$50 and $100&lt;/code&gt; renders &amp;quot;\$50 and $100&amp;quot;. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:This-is-because&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>/project/deep-learning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/deep-learning/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R Example</title>
      <link>/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/post/2015-07-23-r-rmarkdown/</guid>
      <description>&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
